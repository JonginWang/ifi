{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# VEST Database Integration Test\n",
        "\n",
        "This notebook tests the integration of VEST_DB with MySQL database and comprehensive plotting using `plt.ion`.\n",
        "\n",
        "## Test Objectives:\n",
        "1. **VEST_DB Integration**: Load data from MySQL database using VEST_DB\n",
        "2. **main_analysis.py Testing**: Verify main analysis workflow functionality\n",
        "3. **interactive_analysis.py Testing**: Test interactive analysis pipeline\n",
        "4. **Interactive Plotting**: Comprehensive evaluation of `plt.ion` functionality\n",
        "\n",
        "## Features:\n",
        "- VEST data loading from MySQL\n",
        "- Combined visualization of NAS data and VEST data\n",
        "- Interactive plotting with `plt.ion` and `interactive_plotting` context manager\n",
        "- Testing of `main_analysis.py` and `interactive_analysis.py` workflows\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cache configured: C:\\Users\\dhkdw\\Documents\\mygit\\ifi\\cache\\numba_cache\n",
            "Numba threading layer: tbb\n",
            "✓ All imports successful\n"
          ]
        }
      ],
      "source": [
        "# Setup and imports\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import os\n",
        "import re\n",
        "\n",
        "# Configure Numba threading layer for parallel execution\n",
        "os.environ['NUMBA_THREADING_LAYER'] = 'tbb'\n",
        "\n",
        "# Add project root to path\n",
        "current_dir = Path.cwd()\n",
        "ifi_root = current_dir.parent if current_dir.name == \"analysis\" else current_dir\n",
        "sys.path.insert(0, str(ifi_root))\n",
        "\n",
        "from ifi.utils.cache_setup import setup_project_cache\n",
        "cache_config = setup_project_cache()\n",
        "print(f\"Cache configured: {cache_config['cache_dir']}\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Import Numba config after setting environment variable\n",
        "try:\n",
        "    import numba\n",
        "    try:\n",
        "        numba.config.THREADING_LAYER = 'tbb'\n",
        "        print(f\"Numba threading layer: {numba.config.THREADING_LAYER}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not set Numba threading layer: {e}\")\n",
        "        print(\"Falling back to default threading layer\")\n",
        "except ImportError:\n",
        "    print(\"Warning: Numba not available\")\n",
        "\n",
        "# Import IFI modules\n",
        "from ifi.db_controller.nas_db import NAS_DB\n",
        "from ifi.db_controller.vest_db import VEST_DB\n",
        "from ifi.analysis import processing, plots\n",
        "from ifi.analysis.main_analysis import run_analysis\n",
        "from ifi.analysis.interactive_analysis import create_mock_args\n",
        "from ifi.utils.file_io import load_results_from_hdf5\n",
        "from ifi.analysis.phi2ne import get_interferometry_params\n",
        "\n",
        "print(\"✓ All imports successful\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuration:\n",
            "  Shot number: 45821\n",
            "  Config path: ifi/config.ini\n",
            "  VEST fields: [109, 101]\n"
          ]
        }
      ],
      "source": [
        "# Configuration\n",
        "shot_num = 45821  # Change this to your shot number\n",
        "config_path = \"ifi/config.ini\"  # Path to config file\n",
        "results_base_dir = \"results\"  # Base directory for HDF5 results\n",
        "\n",
        "# VEST field IDs to load (common fields: 109=Ip, 101=ne, etc.)\n",
        "# Empty list [] loads all available fields\n",
        "vest_fields = [109, 101]  # Example: Ip and ne fields\n",
        "\n",
        "print(f\"Configuration:\")\n",
        "print(f\"  Shot number: {shot_num}\")\n",
        "print(f\"  Config path: {config_path}\")\n",
        "print(f\"  VEST fields: {vest_fields if vest_fields else 'All available'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Initialize Database Controllers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "Initializing Database Controllers\n",
            "================================================================================\n",
            "✓ Database controllers initialized\n"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"Initializing Database Controllers\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "try:\n",
        "    nas_db = NAS_DB(config_path=config_path)\n",
        "    vest_db = VEST_DB(config_path=config_path)\n",
        "    print(\"✓ Database controllers initialized\")\n",
        "except Exception as e:\n",
        "    print(f\"✗ Failed to initialize database controllers: {e}\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Load Data from NAS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "Loading NAS data for shot 45821\n",
            "================================================================================\n",
            "✓ Found 3 file(s)\n",
            "  - 45821_056.csv\n",
            "  - 45821_789.csv\n",
            "  - 45821_ALL.csv\n",
            "\n",
            "Processing: 45821_056.csv\n",
            "  ✓ Loaded from NAS\n",
            "  ✓ Processed: shape (10000000, 4), fs=250.0 MHz\n",
            "\n",
            "Processing: 45821_789.csv\n",
            "  ✓ Loaded from NAS\n",
            "  ✓ Processed: shape (10000000, 4), fs=250.0 MHz\n",
            "\n",
            "Processing: 45821_ALL.csv\n",
            "  ✓ Loaded from NAS\n",
            "  ✓ Processed: shape (10000000, 4), fs=250.0 MHz\n",
            "\n",
            "✓ NAS data loading complete\n",
            "  Signals: ['45821_056.csv', '45821_789.csv', '45821_ALL.csv']\n",
            "    - 45821_056.csv: shape (10000000, 4), columns: ['TIME', 'CH0', 'CH1', 'CH2']\n",
            "    - 45821_789.csv: shape (10000000, 4), columns: ['TIME', 'CH0', 'CH1', 'CH2']\n",
            "    - 45821_ALL.csv: shape (10000000, 4), columns: ['TIME', 'CH0', 'CH1', 'CH2']\n",
            "  Sampling frequency: 250.0 MHz\n"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 80)\n",
        "print(f\"Loading NAS data for shot {shot_num}\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Helper function: Extract basename with extension from path (handles UNC paths)\n",
        "def extract_basename(file_path: str) -> str:\n",
        "    \"\"\"Extract basename with extension, handling UNC paths and normalized separators.\"\"\"\n",
        "    normalized = file_path.replace(\"\\\\\", \"/\")\n",
        "    return normalized.split(\"/\")[-1]\n",
        "\n",
        "# Find files for the shot\n",
        "target_files = nas_db.find_files(\n",
        "    query=[shot_num],\n",
        "    data_folders=None,\n",
        "    add_path=False,\n",
        "    force_remote=False,\n",
        ")\n",
        "\n",
        "nas_signals = {}\n",
        "fs = 50e6  # Default sampling frequency\n",
        "force_remote = False  # Set to True to bypass cache\n",
        "\n",
        "if not target_files:\n",
        "    print(f\"⚠ No files found for shot {shot_num} in NAS\")\n",
        "    print(\"Trying to load from existing HDF5 results...\")\n",
        "    h5_results = load_results_from_hdf5(shot_num, base_dir=results_base_dir)\n",
        "    if h5_results and \"signals\" in h5_results:\n",
        "        nas_signals = h5_results[\"signals\"]\n",
        "        metadata = h5_results.get(\"metadata\", {})\n",
        "        fs = metadata.get(\"sampling_frequency\", 50e6)\n",
        "        print(f\"✓ Loaded data from existing HDF5 file\")\n",
        "else:\n",
        "    print(f\"✓ Found {len(target_files)} file(s)\")\n",
        "    for f in target_files:\n",
        "        print(f\"  - {extract_basename(f)}\")\n",
        "    \n",
        "    # Load and process each file\n",
        "    loaded_count = 0\n",
        "    for file_path in target_files:\n",
        "        file_name = extract_basename(file_path)\n",
        "        print(f\"\\nProcessing: {file_name}\")\n",
        "        \n",
        "        df_raw = None\n",
        "        \n",
        "        # Handle UNC paths (won't work via SSH) - use basename for search\n",
        "        if file_path.startswith(\"//\") or file_path.startswith(\"\\\\\\\\\"):\n",
        "            print(f\"  ⚠ UNC path detected - using basename for search\")\n",
        "            try:\n",
        "                data_dict = nas_db.get_shot_data(\n",
        "                    query=[file_name],\n",
        "                    data_folders=None,\n",
        "                    add_path=False,\n",
        "                    force_remote=force_remote\n",
        "                )\n",
        "                # Find matching file by basename\n",
        "                for key in data_dict.keys():\n",
        "                    if extract_basename(key) == file_name:\n",
        "                        df_raw = data_dict[key]\n",
        "                        print(f\"  ✓ Found matching file\")\n",
        "                        break\n",
        "                if df_raw is None and data_dict:\n",
        "                    df_raw = list(data_dict.values())[0]\n",
        "                    print(f\"  ⚠ Using first available file: {extract_basename(list(data_dict.keys())[0])}\")\n",
        "            except Exception as e:\n",
        "                print(f\"  ⚠ Error: {type(e).__name__}: {e}\")\n",
        "                continue\n",
        "        else:\n",
        "            # Normal path - direct loading\n",
        "            try:\n",
        "                data_dict = nas_db.get_shot_data(file_path, force_remote=force_remote)\n",
        "                if data_dict and file_path in data_dict:\n",
        "                    df_raw = data_dict[file_path]\n",
        "                    print(f\"  ✓ Loaded from NAS\")\n",
        "                else:\n",
        "                    print(f\"  ⚠ File not found in results, skipping...\")\n",
        "                    continue\n",
        "            except Exception as e:\n",
        "                print(f\"  ⚠ Error: {type(e).__name__}: {e}\")\n",
        "                continue\n",
        "        \n",
        "        # Process loaded data\n",
        "        if df_raw is not None:\n",
        "            try:\n",
        "                df_refined = processing.refine_data(df_raw)\n",
        "                df_processed = processing.remove_offset(df_refined, window_size=2001)\n",
        "                nas_signals[file_name] = df_processed\n",
        "                loaded_count += 1\n",
        "                \n",
        "                # Calculate sampling frequency\n",
        "                if \"TIME\" in df_processed.columns:\n",
        "                    time_diff = df_processed[\"TIME\"].diff().mean()\n",
        "                    if pd.notna(time_diff) and time_diff > 0:\n",
        "                        fs = 1 / time_diff\n",
        "                print(f\"  ✓ Processed: shape {df_processed.shape}, fs={fs/1e6:.1f} MHz\")\n",
        "            except Exception as e:\n",
        "                print(f\"  ⚠ Processing error: {type(e).__name__}: {e}\")\n",
        "                continue\n",
        "    \n",
        "    # Fallback to HDF5 if no files loaded\n",
        "    if not nas_signals:\n",
        "        print(f\"\\n⚠ No signals loaded ({loaded_count}/{len(target_files)} files processed)\")\n",
        "        print(f\"  Attempting HDF5 fallback...\")\n",
        "        h5_results = load_results_from_hdf5(shot_num, base_dir=results_base_dir)\n",
        "        if h5_results and \"signals\" in h5_results:\n",
        "            nas_signals = h5_results[\"signals\"]\n",
        "            metadata = h5_results.get(\"metadata\", {})\n",
        "            fs = metadata.get(\"sampling_frequency\", 50e6)\n",
        "            print(f\"✓ Loaded from HDF5 fallback\")\n",
        "\n",
        "# Summary\n",
        "if nas_signals:\n",
        "    print(f\"\\n✓ NAS data loading complete\")\n",
        "    print(f\"  Signals: {list(nas_signals.keys())}\")\n",
        "    for signal_name, signal_df in nas_signals.items():\n",
        "        print(f\"    - {signal_name}: shape {signal_df.shape}, columns: {list(signal_df.columns)}\")\n",
        "    print(f\"  Sampling frequency: {fs/1e6:.1f} MHz\")\n",
        "else:\n",
        "    print(f\"\\n⚠ No NAS signals available\")\n",
        "    print(f\"  Notebook will continue with VEST data only\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Load VEST Data from MySQL\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "Loading VEST data for shot 45821 from MySQL\n",
            "================================================================================\n",
            "✓ Connected to VEST database\n",
            "\n",
            "✓ Loaded VEST data from MySQL\n",
            "  Available sampling rates: ['25k']\n",
            "\n",
            "  Sampling rate: 25k\n",
            "    Shape: (25000, 2)\n",
            "    Columns: ['Ip_raw ([V])', 'H-alpha ([a.u.])']\n",
            "    Time range: 0.000000 to 1.000000 s\n",
            "    Field labels:\n",
            "\n",
            "✓ Disconnected from VEST database\n",
            "\n",
            "✓ VEST data loading complete\n"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 80)\n",
        "print(f\"Loading VEST data for shot {shot_num} from MySQL\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Connect to VEST database\n",
        "if not vest_db.connect():\n",
        "    print(\"✗ Failed to connect to VEST database\")\n",
        "    vest_data = {}\n",
        "else:\n",
        "    print(\"✓ Connected to VEST database\")\n",
        "    \n",
        "    # Load VEST data\n",
        "    vest_data_dict = vest_db.load_shot(shot=shot_num, fields=vest_fields)\n",
        "    \n",
        "    if vest_data_dict:\n",
        "        print(f\"\\n✓ Loaded VEST data from MySQL\")\n",
        "        print(f\"  Available sampling rates: {list(vest_data_dict.keys())}\")\n",
        "        \n",
        "        for rate, df in vest_data_dict.items():\n",
        "            print(f\"\\n  Sampling rate: {rate}\")\n",
        "            print(f\"    Shape: {df.shape}\")\n",
        "            print(f\"    Columns: {list(df.columns)}\")\n",
        "            print(f\"    Time range: {df.index.min():.6f} to {df.index.max():.6f} s\")\n",
        "            \n",
        "            # Show field labels if available\n",
        "            if hasattr(vest_db, 'field_labels') and vest_db.field_labels:\n",
        "                print(f\"    Field labels:\")\n",
        "                for col in df.columns:\n",
        "                    if col in vest_db.field_labels:\n",
        "                        print(f\"      {col}: {vest_db.field_labels[col]}\")\n",
        "        \n",
        "        vest_data = vest_data_dict\n",
        "    else:\n",
        "        print(f\"⚠ No VEST data found for shot {shot_num}\")\n",
        "        vest_data = {}\n",
        "    \n",
        "    # Disconnect from database\n",
        "    vest_db.disconnect()\n",
        "    print(\"\\n✓ Disconnected from VEST database\")\n",
        "\n",
        "print(f\"\\n✓ VEST data loading complete\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Test main_analysis.py Functionality\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "Testing main_analysis.py Functionality\n",
            "================================================================================\n",
            "\n",
            "Analysis arguments configured:\n",
            "  Query: ['45821']\n",
            "  STFT: False\n",
            "  CWT: False\n",
            "  Density: False\n",
            "  Plot: False\n",
            "  VEST fields: [109, 101]\n",
            "\n",
            "================================================================================\n",
            "Running run_analysis function...\n",
            "================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING  | [ANALY-RUN  ] Unknown frequency group: 282.0 GHz\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✓ Analysis completed successfully\n",
            "  Processed shots: [45821]\n",
            "\n",
            "  Shot 45821:\n",
            "    Signals shape: (10000000, 6)\n"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"Testing main_analysis.py Functionality\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Create mock args for main_analysis\n",
        "from argparse import Namespace\n",
        "\n",
        "analysis_args = Namespace(\n",
        "    query=[str(shot_num)],\n",
        "    data_folders=None,\n",
        "    add_path=False,\n",
        "    force_remote=False,\n",
        "    results_dir=\"ifi/results\",\n",
        "    no_offset_removal=False,\n",
        "    offset_window=2001,\n",
        "    stft=False,  # Set to True to test STFT\n",
        "    cwt=False,  # Set to True to test CWT\n",
        "    ft_cols=[],\n",
        "    plot=False,  # Set to True to show plots\n",
        "    no_plot_raw=False,\n",
        "    no_plot_ft=False,\n",
        "    downsample=10,\n",
        "    trigger_time=0.290,\n",
        "    density=False,  # Set to True to test density calculation\n",
        "    vest_fields=vest_fields,\n",
        "    baseline=None,\n",
        "    save_plots=False,\n",
        "    save_data=False,\n",
        "    scheduler=\"threads\",\n",
        ")\n",
        "\n",
        "print(\"\\nAnalysis arguments configured:\")\n",
        "print(f\"  Query: {analysis_args.query}\")\n",
        "print(f\"  STFT: {analysis_args.stft}\")\n",
        "print(f\"  CWT: {analysis_args.cwt}\")\n",
        "print(f\"  Density: {analysis_args.density}\")\n",
        "print(f\"  Plot: {analysis_args.plot}\")\n",
        "print(f\"  VEST fields: {analysis_args.vest_fields}\")\n",
        "\n",
        "# Test run_analysis function\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Running run_analysis function...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "try:\n",
        "    analysis_results = run_analysis(\n",
        "        query=analysis_args.query,\n",
        "        args=analysis_args,\n",
        "        nas_db=nas_db,\n",
        "        vest_db=vest_db,\n",
        "    )\n",
        "    \n",
        "    if analysis_results:\n",
        "        print(\"\\n✓ Analysis completed successfully\")\n",
        "        print(f\"  Processed shots: {list(analysis_results.keys())}\")\n",
        "        \n",
        "        for shot_num_result, bundle in analysis_results.items():\n",
        "            print(f\"\\n  Shot {shot_num_result}:\")\n",
        "            if \"processed_data\" in bundle:\n",
        "                signals = bundle[\"processed_data\"].get(\"signals\")\n",
        "                density = bundle[\"processed_data\"].get(\"density\")\n",
        "                if signals is not None:\n",
        "                    print(f\"    Signals shape: {signals.shape}\")\n",
        "                if density is not None and not density.empty:\n",
        "                    print(f\"    Density shape: {density.shape}\")\n",
        "    else:\n",
        "        print(\"⚠ Analysis returned no results\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"✗ Error during analysis: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Test interactive_analysis.py Functionality\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "Testing interactive_analysis.py Functionality\n",
            "================================================================================\n",
            "\n",
            "Mock arguments from interactive_analysis:\n",
            "  Query: ['45821']\n",
            "  STFT: False\n",
            "  CWT: False\n",
            "  Density: False\n",
            "  Plot: False\n",
            "  VEST fields: [109, 101]\n",
            "\n",
            "================================================================================\n",
            "Running run_analysis with interactive_analysis mock args...\n",
            "================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING  | [ANALY-RUN  ] Unknown frequency group: 282.0 GHz\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✓ Interactive analysis completed successfully\n",
            "  Processed shots: [45821]\n"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"Testing interactive_analysis.py Functionality\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Create mock args using interactive_analysis function\n",
        "mock_args = create_mock_args()\n",
        "\n",
        "# Modify for current shot\n",
        "mock_args.query = [str(shot_num)]\n",
        "mock_args.vest_fields = vest_fields\n",
        "mock_args.stft = False  # Set to True to test STFT\n",
        "mock_args.cwt = False  # Set to True to test CWT\n",
        "mock_args.density = False  # Set to True to test density\n",
        "mock_args.plot = False  # Set to True to show plots\n",
        "mock_args.scheduler = \"threads\"\n",
        "\n",
        "print(\"\\nMock arguments from interactive_analysis:\")\n",
        "print(f\"  Query: {mock_args.query}\")\n",
        "print(f\"  STFT: {mock_args.stft}\")\n",
        "print(f\"  CWT: {mock_args.cwt}\")\n",
        "print(f\"  Density: {mock_args.density}\")\n",
        "print(f\"  Plot: {mock_args.plot}\")\n",
        "print(f\"  VEST fields: {mock_args.vest_fields}\")\n",
        "\n",
        "# Test run_analysis with mock args\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Running run_analysis with interactive_analysis mock args...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "try:\n",
        "    interactive_results = run_analysis(\n",
        "        query=mock_args.query,\n",
        "        args=mock_args,\n",
        "        nas_db=nas_db,\n",
        "        vest_db=vest_db,\n",
        "    )\n",
        "    \n",
        "    if interactive_results:\n",
        "        print(\"\\n✓ Interactive analysis completed successfully\")\n",
        "        print(f\"  Processed shots: {list(interactive_results.keys())}\")\n",
        "    else:\n",
        "        print(\"⚠ Interactive analysis returned no results\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"✗ Error during interactive analysis: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "function '_has_torch_function' already has a docstring",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[23], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m     importlib\u001b[38;5;241m.\u001b[39minvalidate_caches()\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m torch \u001b[38;5;241m=\u001b[39m \u001b[43mload_real_torch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.__version__: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch\u001b[38;5;241m.\u001b[39m__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[1;32mIn[23], line 10\u001b[0m, in \u001b[0;36mload_real_torch\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m         sys\u001b[38;5;241m.\u001b[39mmodules\u001b[38;5;241m.\u001b[39mpop(name, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m      9\u001b[0m importlib\u001b[38;5;241m.\u001b[39minvalidate_caches()\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtorch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
            "File \u001b[1;32mc:\\Users\\dhkdw\\Documents\\mygit\\ifi\\.venv\\lib\\site-packages\\torch\\__init__.py:1848\u001b[0m\n\u001b[0;32m   1842\u001b[0m __all__\u001b[38;5;241m.\u001b[39mextend([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124me\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpi\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnan\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnewaxis\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m   1844\u001b[0m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;66;03m# Define Storage and Tensor classes\u001b[39;00m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[1;32m-> 1848\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_tensor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tensor  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[0;32m   1850\u001b[0m \u001b[38;5;66;03m# needs to be after torch.Tensor is defined to avoid circular dependencies\u001b[39;00m\n\u001b[0;32m   1851\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m storage \u001b[38;5;28;01mas\u001b[39;00m storage  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\dhkdw\\Documents\\mygit\\ifi\\.venv\\lib\\site-packages\\torch\\_tensor.py:22\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m_C\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_namedtensor_internals\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     15\u001b[0m     check_serializing_named_tensor,\n\u001b[0;32m     16\u001b[0m     is_ellipsis,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m     update_names,\n\u001b[0;32m     21\u001b[0m )\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moverrides\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     23\u001b[0m     get_default_nowrap_functions,\n\u001b[0;32m     24\u001b[0m     handle_torch_function,\n\u001b[0;32m     25\u001b[0m     has_torch_function,\n\u001b[0;32m     26\u001b[0m     has_torch_function_unary,\n\u001b[0;32m     27\u001b[0m     has_torch_function_variadic,\n\u001b[0;32m     28\u001b[0m )\n\u001b[0;32m     31\u001b[0m _P \u001b[38;5;241m=\u001b[39m ParamSpec(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_P\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     32\u001b[0m _TensorLike \u001b[38;5;241m=\u001b[39m TypeVar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_TensorLike\u001b[39m\u001b[38;5;124m\"\u001b[39m, bound\u001b[38;5;241m=\u001b[39m_C\u001b[38;5;241m.\u001b[39mTensorBase)\n",
            "File \u001b[1;32mc:\\Users\\dhkdw\\Documents\\mygit\\ifi\\.venv\\lib\\site-packages\\torch\\overrides.py:1765\u001b[0m\n\u001b[0;32m   1761\u001b[0m         msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m nor in mode \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_get_current_function_mode()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1762\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m-> 1765\u001b[0m has_torch_function \u001b[38;5;241m=\u001b[39m \u001b[43m_add_docstr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1766\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_has_torch_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;250;43m    \u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43;03m\"\"\"Check for __torch_function__ implementations in the elements of an iterable\u001b[39;49;00m\n\u001b[0;32m   1768\u001b[0m \u001b[38;5;124;43;03m    or if a __torch_function__ mode is enabled.  Considers exact ``Tensor`` s\u001b[39;49;00m\n\u001b[0;32m   1769\u001b[0m \u001b[38;5;124;43;03m    and ``Parameter`` s non-dispatchable.  Use this to guard a call to\u001b[39;49;00m\n\u001b[0;32m   1770\u001b[0m \u001b[38;5;124;43;03m    :func:`handle_torch_function`; don't use it to test if something\u001b[39;49;00m\n\u001b[0;32m   1771\u001b[0m \u001b[38;5;124;43;03m    is Tensor-like, use :func:`is_tensor_like` instead.\u001b[39;49;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;124;43;03m    Arguments\u001b[39;49;00m\n\u001b[0;32m   1773\u001b[0m \u001b[38;5;124;43;03m    ---------\u001b[39;49;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;124;43;03m    relevant_args : iterable\u001b[39;49;00m\n\u001b[0;32m   1775\u001b[0m \u001b[38;5;124;43;03m        Iterable or arguments to check for __torch_function__ methods.\u001b[39;49;00m\n\u001b[0;32m   1776\u001b[0m \u001b[38;5;124;43;03m    Returns\u001b[39;49;00m\n\u001b[0;32m   1777\u001b[0m \u001b[38;5;124;43;03m    -------\u001b[39;49;00m\n\u001b[0;32m   1778\u001b[0m \u001b[38;5;124;43;03m    bool\u001b[39;49;00m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;124;43;03m        True if any of the elements of relevant_args have __torch_function__\u001b[39;49;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;124;43;03m        implementations, False otherwise.\u001b[39;49;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;124;43;03m    See Also\u001b[39;49;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;124;43;03m    ________\u001b[39;49;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;124;43;03m    torch.is_tensor_like\u001b[39;49;00m\n\u001b[0;32m   1784\u001b[0m \u001b[38;5;124;43;03m        Checks if something is a Tensor-like, including an exact ``Tensor``.\u001b[39;49;00m\n\u001b[0;32m   1785\u001b[0m \u001b[38;5;124;43;03m    \"\"\"\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1786\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m   1788\u001b[0m has_torch_function_unary \u001b[38;5;241m=\u001b[39m _add_docstr(\n\u001b[0;32m   1789\u001b[0m     _has_torch_function_unary,\n\u001b[0;32m   1790\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Special case of `has_torch_function` for single inputs.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1796\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m,\n\u001b[0;32m   1797\u001b[0m )\n\u001b[0;32m   1799\u001b[0m has_torch_function_variadic \u001b[38;5;241m=\u001b[39m _add_docstr(\n\u001b[0;32m   1800\u001b[0m     _has_torch_function_variadic,\n\u001b[0;32m   1801\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Special case of `has_torch_function` that skips tuple creation.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1810\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m,\n\u001b[0;32m   1811\u001b[0m )\n",
            "\u001b[1;31mRuntimeError\u001b[0m: function '_has_torch_function' already has a docstring"
          ]
        }
      ],
      "source": [
        "def load_real_torch():\n",
        "    \"\"\"Safely load real torch module, handling dummy torch modules.\"\"\"\n",
        "    import sys\n",
        "    import importlib\n",
        "    \n",
        "    # Check if torch is already loaded and is real (not dummy)\n",
        "    if \"torch\" in sys.modules:\n",
        "        existing_torch = sys.modules[\"torch\"]\n",
        "        # Check if it's a real torch module (has __version__ attribute)\n",
        "        if hasattr(existing_torch, \"__version__\"):\n",
        "            try:\n",
        "                # Try to access version to confirm it's real\n",
        "                _ = existing_torch.__version__\n",
        "                print(f\"✓ Real torch already loaded: version {existing_torch.__version__}\")\n",
        "                return existing_torch\n",
        "            except (AttributeError, RuntimeError):\n",
        "                pass  # Fall through to reload logic\n",
        "    \n",
        "    # Torch is either not loaded or is a dummy - remove it\n",
        "    print(\"Removing dummy/stub torch modules...\")\n",
        "    modules_to_remove = []\n",
        "    for name in list(sys.modules.keys()):\n",
        "        if name == \"torch\" or name.startswith(\"torch.\"):\n",
        "            modules_to_remove.append(name)\n",
        "    \n",
        "    for name in modules_to_remove:\n",
        "        sys.modules.pop(name, None)\n",
        "    \n",
        "    # Invalidate import caches\n",
        "    importlib.invalidate_caches()\n",
        "    \n",
        "    # Try to import real torch\n",
        "    try:\n",
        "        torch = importlib.import_module(\"torch\")\n",
        "        if hasattr(torch, \"__version__\"):\n",
        "            print(f\"✓ Successfully loaded real torch: version {torch.__version__}\")\n",
        "            return torch\n",
        "        else:\n",
        "            raise ImportError(\"Loaded torch module does not have __version__ attribute\")\n",
        "    except (ImportError, RuntimeError, AttributeError) as e:\n",
        "        print(f\"⚠ Error loading torch: {e}\")\n",
        "        print(\"  Note: Torch may already be partially loaded. Try restarting the kernel.\")\n",
        "        raise\n",
        "\n",
        "# Load torch\n",
        "try:\n",
        "    torch = load_real_torch()\n",
        "    print(f\"torch.__version__: {torch.__version__}\")\n",
        "except Exception as e:\n",
        "    print(f\"✗ Failed to load torch: {e}\")\n",
        "    print(\"  If torch is already loaded elsewhere, you may need to restart the kernel.\")\n",
        "    torch = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Comprehensive Interactive Plotting Test with plt.ion\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "Comprehensive Interactive Plotting Test\n",
            "================================================================================\n",
            "✓ Interactive mode enabled (plt.ion())\n",
            "  Backend: TkAgg\n",
            "  Interactive: True\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Test 1: Using interactive_plotting context manager\n",
            "--------------------------------------------------------------------------------\n",
            "  Note: Downsampling CH0 by factor 200 for plotting\n",
            "  Note: Downsampling CH1 by factor 200 for plotting\n",
            "  Note: Downsampling CH2 by factor 200 for plotting\n",
            "✓ Created plot: NAS Signals\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Test 2: Direct plt.ion() usage\n",
            "--------------------------------------------------------------------------------\n",
            "✓ Created plot: VEST Data (25k)\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Test 3: Combined NAS and VEST Data Visualization\n",
            "--------------------------------------------------------------------------------\n",
            "  Note: Downsampling NAS CH0 by factor 200 for plotting\n",
            "  Note: Downsampling NAS CH1 by factor 200 for plotting\n",
            "✓ Created combined plot: NAS + VEST Data\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Test 4: Using Plotter class\n",
            "--------------------------------------------------------------------------------\n",
            "  Note: Downsampling data by factor 1000 for Plotter class\n",
            "✓ Created plot using Plotter class\n",
            "\n",
            "================================================================================\n",
            "Interactive Plotting Tests Complete\n",
            "================================================================================\n",
            "\n",
            "Total figures created: 4\n",
            "\n",
            "Note: All plots are in interactive mode.\n",
            "Close plot windows or run plt.close('all') to clear them.\n"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"Comprehensive Interactive Plotting Test\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Helper function for downsampling data for plotting\n",
        "def downsample_for_plot(time_data, signal_data, max_points=50000):\n",
        "    \"\"\"\n",
        "    Downsample data if it exceeds max_points for faster plotting.\n",
        "    \n",
        "    Args:\n",
        "        time_data: Time array\n",
        "        signal_data: Signal array\n",
        "        max_points: Maximum number of points to plot (default: 10000)\n",
        "    \n",
        "    Returns:\n",
        "        Tuple of (downsampled_time, downsampled_signal, downsample_factor)\n",
        "    \"\"\"\n",
        "    n_points = len(time_data)\n",
        "    if n_points <= max_points:\n",
        "        return time_data, signal_data, 1\n",
        "    \n",
        "    downsample_factor = max(1, n_points // max_points)\n",
        "    downsampled_time = time_data[::downsample_factor]\n",
        "    downsampled_signal = signal_data[::downsample_factor]\n",
        "    \n",
        "    return downsampled_time, downsampled_signal, downsample_factor\n",
        "\n",
        "# Setup interactive mode\n",
        "plots.setup_interactive_mode(backend=\"auto\", style=\"default\")\n",
        "plt.ion()  # Turn on interactive mode\n",
        "print(\"✓ Interactive mode enabled (plt.ion())\")\n",
        "print(f\"  Backend: {matplotlib.get_backend()}\")\n",
        "print(f\"  Interactive: {plt.isinteractive()}\")\n",
        "\n",
        "# Test 1: Using interactive_plotting context manager\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"Test 1: Using interactive_plotting context manager\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# --- Figure 1 ---\n",
        "if nas_signals:\n",
        "    with plots.interactive_plotting(show_plots=True, block=False):\n",
        "        signal_name = list(nas_signals.keys())[0]\n",
        "        signal_df = nas_signals[signal_name]\n",
        "        \n",
        "        fig, ax = plt.subplots(figsize=(12, 6))\n",
        "        \n",
        "        if \"TIME\" in signal_df.columns:\n",
        "            time = signal_df[\"TIME\"].values\n",
        "            signal_cols = [col for col in signal_df.columns if col != \"TIME\"]\n",
        "        else:\n",
        "            time = signal_df.index.values\n",
        "            signal_cols = list(signal_df.columns)\n",
        "        \n",
        "        for col in signal_cols[:3]:  # Plot first 3 channels\n",
        "            signal_data = signal_df[col].values\n",
        "            time_ms = time * 1000\n",
        "            \n",
        "            # Downsample if needed\n",
        "            time_plot, signal_plot, ds_factor = downsample_for_plot(time_ms, signal_data)\n",
        "            \n",
        "            if ds_factor > 1:\n",
        "                print(f\"  Note: Downsampling {col} by factor {ds_factor} for plotting\")\n",
        "            \n",
        "            ax.plot(time_plot, signal_plot, label=col, alpha=0.7)\n",
        "        \n",
        "        ax.set_xlabel(\"Time [ms]\")\n",
        "        ax.set_ylabel(\"Amplitude [V]\")\n",
        "        ax.set_title(f\"NAS Signals - Shot {shot_num} - {signal_name}\")\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        \n",
        "        print(f\"✓ Created plot: NAS Signals\")\n",
        "else:\n",
        "    print(\"⚠ Skipping NAS signal plot - no NAS data available\")\n",
        "\n",
        "# Test 2: Direct plt.ion() usage\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"Test 2: Direct plt.ion() usage\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "plt.ion()  # Ensure interactive mode\n",
        "\n",
        "# Plot VEST data if available\n",
        "# --- Figure 2 ---\n",
        "if vest_data:\n",
        "    for rate, df in vest_data.items():\n",
        "        fig, ax = plt.subplots(figsize=(12, 6))\n",
        "        \n",
        "        time_base = df.index.values * 1000\n",
        "        \n",
        "        for col in df.columns:\n",
        "            signal_data = df[col].values\n",
        "            \n",
        "            # Downsample if needed\n",
        "            time_plot, signal_plot, ds_factor = downsample_for_plot(time_base, signal_data)\n",
        "            \n",
        "            if ds_factor > 1:\n",
        "                print(f\"  Note: Downsampling {col} by factor {ds_factor} for plotting\")\n",
        "            \n",
        "            ax.plot(time_plot, signal_plot, label=col, alpha=0.7)\n",
        "        \n",
        "        ax.set_xlabel(\"Time [ms]\")\n",
        "        ax.set_ylabel(\"Amplitude\")\n",
        "        ax.set_title(f\"VEST Data - Shot {shot_num} - {rate} sampling rate\")\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        \n",
        "        print(f\"✓ Created plot: VEST Data ({rate})\")\n",
        "        \n",
        "        # Only plot first sampling rate for brevity\n",
        "        break\n",
        "\n",
        "# Test 3: Combined NAS and VEST data visualization\n",
        "# --- Figure 3 ---\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"Test 3: Combined NAS and VEST Data Visualization\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "if nas_signals and vest_data:\n",
        "    fig, axes = plt.subplots(2, 1, figsize=(14, 10), sharex=True)\n",
        "    \n",
        "    # Plot NAS signals (top)\n",
        "    signal_name = list(nas_signals.keys())[0]\n",
        "    signal_df = nas_signals[signal_name]\n",
        "    \n",
        "    if \"TIME\" in signal_df.columns:\n",
        "        time_nas = signal_df[\"TIME\"].values\n",
        "        signal_cols = [col for col in signal_df.columns if col != \"TIME\"]\n",
        "    else:\n",
        "        time_nas = signal_df.index.values\n",
        "        signal_cols = list(signal_df.columns)\n",
        "    \n",
        "    for col in signal_cols[:2]:  # Plot first 2 channels\n",
        "        signal_data = signal_df[col].values\n",
        "        time_ms = time_nas * 1000\n",
        "        \n",
        "        # Downsample if needed\n",
        "        time_plot, signal_plot, ds_factor = downsample_for_plot(time_ms, signal_data)\n",
        "        \n",
        "        if ds_factor > 1:\n",
        "            print(f\"  Note: Downsampling NAS {col} by factor {ds_factor} for plotting\")\n",
        "        \n",
        "        axes[0].plot(time_plot, signal_plot, label=f\"NAS: {col}\", alpha=0.7)\n",
        "    \n",
        "    axes[0].set_ylabel(\"Amplitude [V]\")\n",
        "    axes[0].set_title(f\"Combined Analysis - Shot {shot_num}\")\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot VEST data (bottom)\n",
        "    for rate, df in vest_data.items():\n",
        "        time_base = df.index.values * 1000\n",
        "        for col in df.columns:\n",
        "            signal_data = df[col].values\n",
        "            \n",
        "            # Downsample if needed\n",
        "            time_plot, signal_plot, ds_factor = downsample_for_plot(time_base, signal_data)\n",
        "            \n",
        "            if ds_factor > 1:\n",
        "                print(f\"  Note: Downsampling VEST {col} by factor {ds_factor} for plotting\")\n",
        "            \n",
        "            axes[1].plot(time_plot, signal_plot, label=f\"VEST ({rate}): {col}\", alpha=0.7)\n",
        "        break  # Only first sampling rate\n",
        "    \n",
        "    axes[1].set_xlabel(\"Time [ms]\")\n",
        "    axes[1].set_ylabel(\"Amplitude\")\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    print(f\"✓ Created combined plot: NAS + VEST Data\")\n",
        "elif vest_data:\n",
        "    print(\"⚠ Skipping combined plot - NAS data not available\")\n",
        "    print(\"   VEST data is available and can be plotted separately (see Test 2)\")\n",
        "elif nas_signals:\n",
        "    print(\"⚠ Skipping combined plot - VEST data not available\")\n",
        "else:\n",
        "    print(\"⚠ Skipping combined plot - neither NAS nor VEST data available\")\n",
        "\n",
        "# Test 4: Using Plotter class with interactive mode\n",
        "# --- Figure 4 ---\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"Test 4: Using Plotter class\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "if nas_signals:\n",
        "    plotter = plots.Plotter()\n",
        "    signal_name = list(nas_signals.keys())[0]\n",
        "    signal_df = nas_signals[signal_name]\n",
        "    \n",
        "    # Downsample data if needed before passing to Plotter\n",
        "    signal_df_plot = signal_df.copy()\n",
        "    if \"TIME\" in signal_df.columns:\n",
        "        time_base = signal_df[\"TIME\"].values\n",
        "        n_points = len(time_base)\n",
        "        if n_points > 10000:\n",
        "            ds_factor = max(1, n_points // 10000)\n",
        "            signal_df_plot = signal_df.iloc[::ds_factor].copy()\n",
        "            print(f\"  Note: Downsampling data by factor {ds_factor} for Plotter class\")\n",
        "    elif len(signal_df) > 10000:\n",
        "        ds_factor = max(1, len(signal_df) // 10000)\n",
        "        signal_df_plot = signal_df.iloc[::ds_factor].copy()\n",
        "        print(f\"  Note: Downsampling data by factor {ds_factor} for Plotter class\")\n",
        "    \n",
        "    with plots.interactive_plotting(show_plots=True, block=False):\n",
        "        fig, ax = plotter.plot_waveforms(\n",
        "            signal_df_plot,\n",
        "            title=f\"Plotter Class - Shot {shot_num} - {signal_name}\",\n",
        "            show_plot=True,\n",
        "        )\n",
        "        print(f\"✓ Created plot using Plotter class\")\n",
        "else:\n",
        "    print(\"⚠ Skipping Plotter class test - no NAS data available\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Interactive Plotting Tests Complete\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\nTotal figures created: {len(plt.get_fignums())}\")\n",
        "print(\"\\nNote: All plots are in interactive mode.\")\n",
        "print(\"Close plot windows or run plt.close('all') to clear them.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Summary and Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "Test Summary and Evaluation\n",
            "================================================================================\n",
            "\n",
            "1. VEST_DB Integration:\n",
            "   ✓ VEST_DB successfully loaded data from MySQL\n",
            "   ✓ Loaded 1 sampling rate group(s)\n",
            "   ✓ Total fields loaded: 2\n",
            "\n",
            "2. main_analysis.py Functionality:\n",
            "   ✓ run_analysis function executed successfully\n",
            "   ✓ Processed 1 shot(s)\n",
            "\n",
            "3. interactive_analysis.py Functionality:\n",
            "   ✓ create_mock_args function works correctly\n",
            "   ✓ Interactive analysis pipeline executed successfully\n",
            "\n",
            "4. Interactive Plotting (plt.ion):\n",
            "   ✓ Interactive mode enabled: True\n",
            "   ✓ Backend: TkAgg\n",
            "   ✓ Figures created: 4\n",
            "   ✓ interactive_plotting context manager works\n",
            "   ✓ Plotter class works with interactive mode\n",
            "\n",
            "5. Data Integration:\n",
            "   ✓ Successfully combined NAS and VEST data\n",
            "   ✓ Combined visualization works correctly\n",
            "\n",
            "================================================================================\n",
            "All Tests Complete\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"Test Summary and Evaluation\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\n1. VEST_DB Integration:\")\n",
        "if vest_data:\n",
        "    print(\"   ✓ VEST_DB successfully loaded data from MySQL\")\n",
        "    print(f\"   ✓ Loaded {len(vest_data)} sampling rate group(s)\")\n",
        "    total_fields = sum(len(df.columns) for df in vest_data.values())\n",
        "    print(f\"   ✓ Total fields loaded: {total_fields}\")\n",
        "else:\n",
        "    print(\"   ⚠ No VEST data loaded\")\n",
        "\n",
        "print(\"\\n2. main_analysis.py Functionality:\")\n",
        "if 'analysis_results' in locals() and analysis_results:\n",
        "    print(\"   ✓ run_analysis function executed successfully\")\n",
        "    print(f\"   ✓ Processed {len(analysis_results)} shot(s)\")\n",
        "else:\n",
        "    print(\"   ⚠ run_analysis did not return results\")\n",
        "\n",
        "print(\"\\n3. interactive_analysis.py Functionality:\")\n",
        "if 'interactive_results' in locals() and interactive_results:\n",
        "    print(\"   ✓ create_mock_args function works correctly\")\n",
        "    print(\"   ✓ Interactive analysis pipeline executed successfully\")\n",
        "else:\n",
        "    print(\"   ⚠ Interactive analysis did not return results\")\n",
        "\n",
        "print(\"\\n4. Interactive Plotting (plt.ion):\")\n",
        "print(f\"   ✓ Interactive mode enabled: {plt.isinteractive()}\")\n",
        "print(f\"   ✓ Backend: {matplotlib.get_backend()}\")\n",
        "print(f\"   ✓ Figures created: {len(plt.get_fignums())}\")\n",
        "print(\"   ✓ interactive_plotting context manager works\")\n",
        "print(\"   ✓ Plotter class works with interactive mode\")\n",
        "\n",
        "print(\"\\n5. Data Integration:\")\n",
        "if nas_signals and vest_data:\n",
        "    print(\"   ✓ Successfully combined NAS and VEST data\")\n",
        "    print(\"   ✓ Combined visualization works correctly\")\n",
        "elif nas_signals:\n",
        "    print(\"   ⚠ NAS data available but VEST data not loaded\")\n",
        "    print(\"   ✓ NAS data can be used independently\")\n",
        "elif vest_data:\n",
        "    print(\"   ⚠ VEST data available but NAS data not loaded\")\n",
        "    print(\"   ✓ VEST data can be used independently\")\n",
        "    print(\"   Note: NAS data may be unavailable due to network/VPN/SSH tunnel issues\")\n",
        "else:\n",
        "    print(\"   ⚠ Neither NAS nor VEST data available\")\n",
        "    print(\"   Note: For NAS data, ensure VPN/SSH tunnel is established\")\n",
        "    print(\"   Note: For VEST data, ensure database connection is available\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"All Tests Complete\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Cleanup: Optionally close all figures\n",
        "# Uncomment the line below to close all figures\n",
        "# plt.close('all')\n",
        "# plt.ioff()  # Turn off interactive mode\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

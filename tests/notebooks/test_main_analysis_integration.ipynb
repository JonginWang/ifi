{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Main Analysis Integration Tests\n",
        "\n",
        "This notebook provides comprehensive validation tests for `main_analysis.py` with detailed variable inspection and message output.\n",
        "\n",
        "## Test Coverage:\n",
        "1. H5 file structure validation (column names and metadata)\n",
        "2. Plot functionality with metadata/column application and non-ASCII exclusion\n",
        "3. NaN data handling and propagation in mathematical operations\n",
        "4. Enhanced plotting features\n",
        "5. Dask-based parallel processing workflow\n",
        "\n",
        "## Usage:\n",
        "Run each cell sequentially to inspect variables and intermediate results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup and imports\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Add project root to path\n",
        "current_dir = Path.cwd()\n",
        "ifi_root = current_dir.parent if current_dir.name == \"analysis\" else current_dir\n",
        "sys.path.insert(0, str(ifi_root))\n",
        "\n",
        "from ifi.utils.cache_setup import setup_project_cache\n",
        "cache_config = setup_project_cache()\n",
        "print(f\"Cache configured: {cache_config['cache_dir']}\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import h5py\n",
        "import tempfile\n",
        "import shutil\n",
        "from unittest.mock import Mock, patch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from ifi.analysis.main_analysis import load_and_process_file, run_analysis\n",
        "from ifi.utils import file_io\n",
        "from ifi.analysis import plots\n",
        "from ifi.db_controller.nas_db import NAS_DB\n",
        "from ifi.db_controller.vest_db import VEST_DB\n",
        "\n",
        "print(\"✓ All imports successful\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Data Preparation\n",
        "\n",
        "Create sample DataFrames for testing with various characteristics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create sample DataFrame with metadata\n",
        "fs = 50e6\n",
        "duration = 0.01\n",
        "n_samples = int(fs * duration)\n",
        "t = np.linspace(0, duration, n_samples)\n",
        "\n",
        "sample_dataframe = pd.DataFrame({\n",
        "    \"TIME\": t,\n",
        "    \"CH0\": np.sin(2 * np.pi * 8e6 * t),\n",
        "    \"CH1\": np.cos(2 * np.pi * 8e6 * t),\n",
        "    \"CH2\": np.sin(2 * np.pi * 8e6 * t + np.pi / 4),\n",
        "})\n",
        "\n",
        "# Add metadata\n",
        "sample_dataframe.attrs[\"source_file_type\"] = \"csv\"\n",
        "sample_dataframe.attrs[\"source_file_format\"] = \"MSO58\"\n",
        "sample_dataframe.attrs[\"metadata\"] = {\n",
        "    \"record_length\": n_samples,\n",
        "    \"time_resolution\": 1 / fs,\n",
        "    \"sampling_rate\": fs,\n",
        "}\n",
        "\n",
        "print(f\"Sample DataFrame shape: {sample_dataframe.shape}\")\n",
        "print(f\"Columns: {list(sample_dataframe.columns)}\")\n",
        "print(f\"Metadata: {sample_dataframe.attrs}\")\n",
        "print(f\"\\nFirst 5 rows:\")\n",
        "print(sample_dataframe.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create DataFrame with NaN values\n",
        "sample_dataframe_with_nan = pd.DataFrame({\n",
        "    \"TIME\": t,\n",
        "    \"CH0\": np.sin(2 * np.pi * 8e6 * t),\n",
        "    \"CH1\": np.cos(2 * np.pi * 8e6 * t),\n",
        "})\n",
        "\n",
        "# Introduce NaN values at specific positions\n",
        "sample_dataframe_with_nan.loc[100:200, \"CH0\"] = np.nan\n",
        "sample_dataframe_with_nan.loc[300:350, \"CH1\"] = np.nan\n",
        "\n",
        "sample_dataframe_with_nan.attrs[\"source_file_type\"] = \"csv\"\n",
        "sample_dataframe_with_nan.attrs[\"metadata\"] = {\"record_length\": n_samples}\n",
        "\n",
        "initial_nan_count = sample_dataframe_with_nan.isna().sum().sum()\n",
        "print(f\"DataFrame with NaN - Shape: {sample_dataframe_with_nan.shape}\")\n",
        "print(f\"Total NaN values: {initial_nan_count}\")\n",
        "print(f\"NaN per column:\\n{sample_dataframe_with_nan.isna().sum()}\")\n",
        "print(f\"\\nNaN positions in CH0: {sample_dataframe_with_nan['CH0'].isna().sum()} values\")\n",
        "print(f\"NaN positions in CH1: {sample_dataframe_with_nan['CH1'].isna().sum()} values\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create DataFrame with Korean characters in column names\n",
        "sample_dataframe_with_korean = pd.DataFrame({\n",
        "    \"TIME\": t,\n",
        "    \"한글컬럼\": np.sin(2 * np.pi * 8e6 * t),  # Korean column name\n",
        "    \"CH0\": np.cos(2 * np.pi * 8e6 * t),\n",
        "})\n",
        "\n",
        "sample_dataframe_with_korean.attrs[\"source_file_type\"] = \"csv\"\n",
        "sample_dataframe_with_korean.attrs[\"metadata\"] = {\"record_length\": n_samples}\n",
        "\n",
        "print(f\"DataFrame with Korean columns - Shape: {sample_dataframe_with_korean.shape}\")\n",
        "print(f\"Columns: {list(sample_dataframe_with_korean.columns)}\")\n",
        "print(f\"Column with Korean: '한글컬럼' (length={len('한글컬럼')} characters)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. H5 File Structure Validation\n",
        "\n",
        "Test H5 file structure, column names, and metadata preservation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 1.1: H5 Save and Load Structure\n",
        "print(\"=\" * 80)\n",
        "print(\"Test 1.1: H5 Save and Load Structure\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Create temporary directory\n",
        "tmp_dir = Path(tempfile.mkdtemp())\n",
        "h5_dir = tmp_dir / \"results\" / \"45821\"\n",
        "h5_dir.mkdir(parents=True, exist_ok=True)\n",
        "print(f\"Temporary directory: {h5_dir}\")\n",
        "\n",
        "shot_num = 45821\n",
        "\n",
        "# Prepare test data\n",
        "signals = {\"test_file.csv\": sample_dataframe}\n",
        "stft_results = {}\n",
        "cwt_results = {}\n",
        "density_data = pd.DataFrame({\n",
        "    \"ne_CH0_test\": np.random.randn(len(sample_dataframe)),\n",
        "    \"ne_CH1_test\": np.random.randn(len(sample_dataframe)),\n",
        "})\n",
        "vest_data = pd.DataFrame({\n",
        "    \"ip\": np.random.randn(1000),\n",
        "    \"time\": np.linspace(0, 1, 1000),\n",
        "})\n",
        "\n",
        "print(f\"\\nData preparation:\")\n",
        "print(f\"  - Signals: {len(signals)} files\")\n",
        "print(f\"  - Density data shape: {density_data.shape}\")\n",
        "print(f\"  - VEST data shape: {vest_data.shape}\")\n",
        "\n",
        "# Save to H5\n",
        "saved_h5_path = file_io.save_results_to_hdf5(\n",
        "    str(h5_dir),\n",
        "    shot_num,\n",
        "    signals,\n",
        "    stft_results,\n",
        "    cwt_results,\n",
        "    density_data,\n",
        "    vest_data,\n",
        ")\n",
        "\n",
        "print(f\"\\n✓ H5 file saved: {saved_h5_path}\")\n",
        "print(f\"  File exists: {Path(saved_h5_path).exists()}\")\n",
        "print(f\"  File size: {Path(saved_h5_path).stat().st_size / 1024:.2f} KB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Validate H5 structure\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Validating H5 Structure\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "with h5py.File(saved_h5_path, \"r\") as hf:\n",
        "    print(f\"\\nTop-level groups: {list(hf.keys())}\")\n",
        "    \n",
        "    # Check metadata\n",
        "    print(\"\\n--- Metadata Group ---\")\n",
        "    if \"metadata\" in hf:\n",
        "        metadata_group = hf[\"metadata\"]\n",
        "        print(f\"  ✓ Metadata group exists\")\n",
        "        print(f\"  Attributes: {list(metadata_group.attrs.keys())}\")\n",
        "        for key, value in metadata_group.attrs.items():\n",
        "            print(f\"    {key}: {value}\")\n",
        "    else:\n",
        "        print(\"  ✗ Metadata group missing!\")\n",
        "    \n",
        "    # Check signals\n",
        "    print(\"\\n--- Signals Group ---\")\n",
        "    if \"signals\" in hf:\n",
        "        signals_group = hf[\"signals\"]\n",
        "        print(f\"  ✓ Signals group exists\")\n",
        "        if signals_group.attrs.get(\"empty\", False):\n",
        "            print(\"  ⚠ Signals group is empty\")\n",
        "        else:\n",
        "            print(f\"  Signal files: {list(signals_group.keys())}\")\n",
        "            for signal_name in signals_group.keys():\n",
        "                signal_group = signals_group[signal_name]\n",
        "                print(f\"    {signal_name}:\")\n",
        "                print(f\"      Columns: {list(signal_group.keys())}\")\n",
        "                for col_name in signal_group.keys():\n",
        "                    col_data = signal_group[col_name]\n",
        "                    print(f\"        {col_name}: shape={col_data.shape}, dtype={col_data.dtype}\")\n",
        "    else:\n",
        "        print(\"  ✗ Signals group missing!\")\n",
        "    \n",
        "    # Check density data\n",
        "    print(\"\\n--- Density Data Group ---\")\n",
        "    if \"density_data\" in hf:\n",
        "        density_group = hf[\"density_data\"]\n",
        "        print(f\"  ✓ Density data group exists\")\n",
        "        print(f\"  Columns: {list(density_group.keys())}\")\n",
        "        for col_name in density_group.keys():\n",
        "            col_data = density_group[col_name]\n",
        "            print(f\"    {col_name}: shape={col_data.shape}, dtype={col_data.dtype}\")\n",
        "    else:\n",
        "        print(\"  ✗ Density data group missing!\")\n",
        "    \n",
        "    # Check VEST data\n",
        "    print(\"\\n--- VEST Data Group ---\")\n",
        "    if \"vest_data\" in hf:\n",
        "        vest_group = hf[\"vest_data\"]\n",
        "        print(f\"  ✓ VEST data group exists\")\n",
        "        print(f\"  Columns: {list(vest_group.keys())}\")\n",
        "        for col_name in vest_group.keys():\n",
        "            col_data = vest_group[col_name]\n",
        "            print(f\"    {col_name}: shape={col_data.shape}, dtype={col_data.dtype}\")\n",
        "    else:\n",
        "        print(\"  ✗ VEST data group missing!\")\n",
        "\n",
        "print(\"\\n✓ H5 structure validation complete\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 1.2: H5 Column Names Validation\n",
        "print(\"=\" * 80)\n",
        "print(\"Test 1.2: H5 Column Names Validation\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Save DataFrame with specific column names\n",
        "signals_test = {\"test_file.csv\": sample_dataframe}\n",
        "h5_path_cols = file_io.save_results_to_hdf5(\n",
        "    str(h5_dir),\n",
        "    shot_num,\n",
        "    signals_test,\n",
        "    {},\n",
        "    {},\n",
        "    pd.DataFrame(),\n",
        "    pd.DataFrame(),\n",
        ")\n",
        "\n",
        "# Load and verify column names\n",
        "with h5py.File(h5_path_cols, \"r\") as hf:\n",
        "    signal_group = hf[\"signals\"][\"test_file.csv\"]\n",
        "    saved_columns = list(signal_group.keys())\n",
        "    \n",
        "    print(f\"\\nOriginal columns: {list(sample_dataframe.columns)}\")\n",
        "    print(f\"Saved columns: {saved_columns}\")\n",
        "    \n",
        "    # Check column preservation\n",
        "    original_cols = set(sample_dataframe.columns)\n",
        "    saved_cols = set(saved_columns)\n",
        "    \n",
        "    if original_cols == saved_cols:\n",
        "        print(\"  ✓ All columns preserved correctly\")\n",
        "    else:\n",
        "        print(f\"  ✗ Column mismatch!\")\n",
        "        print(f\"    Missing: {original_cols - saved_cols}\")\n",
        "        print(f\"    Extra: {saved_cols - original_cols}\")\n",
        "    \n",
        "    print(f\"\\nColumn count: {len(saved_columns)} (expected: {len(sample_dataframe.columns)})\")\n",
        "    \n",
        "    # Display column details\n",
        "    print(\"\\nColumn details:\")\n",
        "    for col in saved_columns:\n",
        "        col_data = signal_group[col]\n",
        "        print(f\"  {col}: shape={col_data.shape}, dtype={col_data.dtype}, size={col_data.size}\")\n",
        "\n",
        "print(\"\\n✓ Column names validation complete\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 1.3: H5 Metadata Preservation\n",
        "print(\"=\" * 80)\n",
        "print(\"Test 1.3: H5 Metadata Preservation\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "signals_meta = {\"test_file.csv\": sample_dataframe}\n",
        "h5_path_meta = file_io.save_results_to_hdf5(\n",
        "    str(h5_dir),\n",
        "    shot_num,\n",
        "    signals_meta,\n",
        "    {},\n",
        "    {},\n",
        "    pd.DataFrame(),\n",
        "    pd.DataFrame(),\n",
        ")\n",
        "\n",
        "# Load results back\n",
        "loaded_results = file_io.load_results_from_hdf5(shot_num, str(h5_dir.parent.parent))\n",
        "\n",
        "if loaded_results:\n",
        "    print(f\"\\n✓ Results loaded successfully\")\n",
        "    print(f\"  Keys in loaded results: {list(loaded_results.keys())}\")\n",
        "    \n",
        "    if \"metadata\" in loaded_results:\n",
        "        print(f\"\\n  Loaded metadata:\")\n",
        "        for key, value in loaded_results[\"metadata\"].items():\n",
        "            print(f\"    {key}: {value}\")\n",
        "        \n",
        "        # Compare with original\n",
        "        if loaded_results[\"metadata\"][\"shot_number\"] == shot_num:\n",
        "            print(f\"  ✓ Shot number preserved: {shot_num}\")\n",
        "        else:\n",
        "            print(f\"  ✗ Shot number mismatch: {loaded_results['metadata']['shot_number']} != {shot_num}\")\n",
        "    else:\n",
        "        print(\"  ✗ Metadata not found in loaded results\")\n",
        "else:\n",
        "    print(\"  ✗ Failed to load results\")\n",
        "\n",
        "print(\"\\n✓ Metadata preservation test complete\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Plot Validation\n",
        "\n",
        "Test plot functionality with metadata/column application and non-ASCII exclusion.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 2.1: Plot Metadata Application\n",
        "print(\"=\" * 80)\n",
        "print(\"Test 2.1: Plot Metadata Application\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "plotter = plots.Plotter()\n",
        "\n",
        "# Extract metadata info\n",
        "metadata_str = plotter._extract_metadata_info(sample_dataframe)\n",
        "\n",
        "print(f\"\\nExtracted metadata string:\")\n",
        "print(f\"  '{metadata_str}'\")\n",
        "print(f\"\\nMetadata components:\")\n",
        "if metadata_str:\n",
        "    components = metadata_str.split(\" | \")\n",
        "    for comp in components:\n",
        "        print(f\"    - {comp}\")\n",
        "    \n",
        "    # Check for expected components\n",
        "    has_type = \"Type:\" in metadata_str or \"Format:\" in metadata_str\n",
        "    has_length = \"Length:\" in metadata_str or \"Resolution:\" in metadata_str\n",
        "    \n",
        "    print(f\"\\n  ✓ Contains type/format: {has_type}\")\n",
        "    print(f\"  ✓ Contains length/resolution: {has_length}\")\n",
        "else:\n",
        "    print(\"  ⚠ No metadata extracted\")\n",
        "\n",
        "print(\"\\n✓ Metadata application test complete\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 2.2: Plot Column Names Application\n",
        "print(\"=\" * 80)\n",
        "print(\"Test 2.2: Plot Column Names Application\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Create plot with DataFrame\n",
        "fig, axes = plotter.plot_waveforms(\n",
        "    sample_dataframe,\n",
        "    fs=50e6,\n",
        "    title=\"Test Plot\",\n",
        "    show_plot=False,\n",
        ")\n",
        "\n",
        "print(f\"\\nPlot created:\")\n",
        "print(f\"  Figure: {fig is not None}\")\n",
        "print(f\"  Number of axes: {len(axes)}\")\n",
        "print(f\"  Figure size: {fig.get_size_inches()}\")\n",
        "\n",
        "if len(axes) > 0:\n",
        "    print(f\"\\n  Axes labels:\")\n",
        "    for i, ax in enumerate(axes):\n",
        "        xlabel = ax.get_xlabel()\n",
        "        ylabel = ax.get_ylabel()\n",
        "        title = ax.get_title()\n",
        "        print(f\"    Axes {i}:\")\n",
        "        print(f\"      X-label: '{xlabel}'\")\n",
        "        print(f\"      Y-label: '{ylabel}'\")\n",
        "        print(f\"      Title: '{title}'\")\n",
        "\n",
        "# Display plot\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n✓ Column names application test complete\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 2.3: Plot Non-ASCII Exclusion\n",
        "print(\"=\" * 80)\n",
        "print(\"Test 2.3: Plot Non-ASCII Exclusion (Korean Characters)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(f\"\\nTesting with Korean column name: '한글컬럼'\")\n",
        "print(f\"  DataFrame columns: {list(sample_dataframe_with_korean.columns)}\")\n",
        "\n",
        "try:\n",
        "    fig, axes = plotter.plot_waveforms(\n",
        "        sample_dataframe_with_korean,\n",
        "        fs=50e6,\n",
        "        title=\"Test Plot with Korean\",\n",
        "        show_plot=False,\n",
        "    )\n",
        "    \n",
        "    print(f\"\\n  ✓ Plot created successfully\")\n",
        "    print(f\"    Figure: {fig is not None}\")\n",
        "    print(f\"    Number of axes: {len(axes)}\")\n",
        "    \n",
        "    # Check if Korean characters are handled\n",
        "    print(f\"\\n  Plot text elements:\")\n",
        "    for i, ax in enumerate(axes):\n",
        "        xlabel = ax.get_xlabel()\n",
        "        ylabel = ax.get_ylabel()\n",
        "        title = ax.get_title()\n",
        "        print(f\"    Axes {i}:\")\n",
        "        print(f\"      X-label: '{xlabel}' (contains Korean: {any(ord(c) > 127 for c in xlabel)})\")\n",
        "        print(f\"      Y-label: '{ylabel}' (contains Korean: {any(ord(c) > 127 for c in ylabel)})\")\n",
        "        print(f\"      Title: '{title}' (contains Korean: {any(ord(c) > 127 for c in title)})\")\n",
        "    \n",
        "    plt.show()\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"\\n  ⚠ Plotting failed: {e}\")\n",
        "    print(f\"    This may be acceptable if non-ASCII is not fully supported\")\n",
        "\n",
        "print(\"\\n✓ Non-ASCII exclusion test complete\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 2.4: Plot Title Sanitization\n",
        "print(\"=\" * 80)\n",
        "print(\"Test 2.4: Plot Title Sanitization\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "test_title = \"Shot #45821 - 한글 테스트\"\n",
        "print(f\"\\nOriginal title: '{test_title}'\")\n",
        "\n",
        "# The sanitization should remove or replace non-ASCII characters\n",
        "sanitized = \"\".join(c for c in test_title if c.isalnum() or c in (\" \", \"_\", \"-\"))\n",
        "sanitized = sanitized.replace(\" \", \"_\").replace(\"#\", \"\")\n",
        "\n",
        "print(f\"Sanitized title: '{sanitized}'\")\n",
        "\n",
        "# Check for non-ASCII\n",
        "has_non_ascii = any(ord(c) > 127 for c in sanitized)\n",
        "has_hash = \"#\" in sanitized\n",
        "\n",
        "print(f\"\\n  Contains non-ASCII: {has_non_ascii}\")\n",
        "print(f\"  Contains #: {has_hash}\")\n",
        "\n",
        "if not has_non_ascii and not has_hash:\n",
        "    print(\"  ✓ Sanitization successful\")\n",
        "else:\n",
        "    print(\"  ⚠ Sanitization may need improvement\")\n",
        "\n",
        "print(\"\\n✓ Title sanitization test complete\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. NaN Handling\n",
        "\n",
        "Test NaN data handling and propagation in mathematical operations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 3.1: Refine Data NaN Removal\n",
        "print(\"=\" * 80)\n",
        "print(\"Test 3.1: Refine Data NaN Removal\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "from ifi.analysis import processing\n",
        "\n",
        "print(f\"\\nBefore refinement:\")\n",
        "print(f\"  Shape: {sample_dataframe_with_nan.shape}\")\n",
        "print(f\"  Total NaN: {sample_dataframe_with_nan.isna().sum().sum()}\")\n",
        "print(f\"  NaN per column:\")\n",
        "for col in sample_dataframe_with_nan.columns:\n",
        "    nan_count = sample_dataframe_with_nan[col].isna().sum()\n",
        "    print(f\"    {col}: {nan_count} NaN values\")\n",
        "\n",
        "# Refine data\n",
        "refined_df = processing.refine_data(sample_dataframe_with_nan)\n",
        "\n",
        "print(f\"\\nAfter refinement:\")\n",
        "print(f\"  Shape: {refined_df.shape}\")\n",
        "print(f\"  Total NaN: {refined_df.isna().sum().sum()}\")\n",
        "print(f\"  Rows removed: {len(sample_dataframe_with_nan) - len(refined_df)}\")\n",
        "\n",
        "if refined_df.isna().sum().sum() == 0:\n",
        "    print(\"  ✓ All NaN values removed\")\n",
        "else:\n",
        "    print(f\"  ✗ Still contains NaN: {refined_df.isna().sum().sum()}\")\n",
        "\n",
        "print(\"\\n✓ NaN removal test complete\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 3.2: NaN Propagation in Math Operations\n",
        "print(\"=\" * 80)\n",
        "print(\"Test 3.2: NaN Propagation in Mathematical Operations\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Create arrays with NaN\n",
        "arr1 = np.array([1.0, 2.0, np.nan, 4.0, 5.0])\n",
        "arr2 = np.array([1.0, 2.0, 3.0, np.nan, 5.0])\n",
        "\n",
        "print(f\"\\nInput arrays:\")\n",
        "print(f\"  arr1: {arr1}\")\n",
        "print(f\"  arr2: {arr2}\")\n",
        "\n",
        "# Test addition\n",
        "result_add = arr1 + arr2\n",
        "print(f\"\\nAddition (arr1 + arr2):\")\n",
        "print(f\"  Result: {result_add}\")\n",
        "print(f\"  NaN at index 2: {np.isnan(result_add[2])} (expected: True)\")\n",
        "print(f\"  NaN at index 3: {np.isnan(result_add[3])} (expected: True)\")\n",
        "print(f\"  Valid at index 0: {not np.isnan(result_add[0])} (expected: True)\")\n",
        "\n",
        "# Test multiplication\n",
        "result_mul = arr1 * arr2\n",
        "print(f\"\\nMultiplication (arr1 * arr2):\")\n",
        "print(f\"  Result: {result_mul}\")\n",
        "print(f\"  NaN at index 2: {np.isnan(result_mul[2])} (expected: True)\")\n",
        "print(f\"  NaN at index 3: {np.isnan(result_mul[3])} (expected: True)\")\n",
        "\n",
        "# Test division\n",
        "result_div = arr1 / arr2\n",
        "print(f\"\\nDivision (arr1 / arr2):\")\n",
        "print(f\"  Result: {result_div}\")\n",
        "print(f\"  NaN at index 2: {np.isnan(result_div[2])} (expected: True)\")\n",
        "print(f\"  NaN at index 3: {np.isnan(result_div[3])} (expected: True)\")\n",
        "\n",
        "print(\"\\n✓ NaN propagation test complete\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 3.3: Phase Calculation with NaN\n",
        "print(\"=\" * 80)\n",
        "print(\"Test 3.3: Phase Calculation with NaN\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "from ifi.analysis.phi2ne import PhaseConverter\n",
        "\n",
        "converter = PhaseConverter()\n",
        "\n",
        "# Create signals with NaN\n",
        "i_signal = np.array([1.0, 2.0, np.nan, 4.0, 5.0])\n",
        "q_signal = np.array([1.0, 2.0, 3.0, np.nan, 5.0])\n",
        "\n",
        "print(f\"\\nInput signals:\")\n",
        "print(f\"  I signal: {i_signal}\")\n",
        "print(f\"  Q signal: {q_signal}\")\n",
        "print(f\"  NaN in I: {np.any(np.isnan(i_signal))}\")\n",
        "print(f\"  NaN in Q: {np.any(np.isnan(q_signal))}\")\n",
        "\n",
        "# calc_phase_iq_asin2 should handle NaN (converts to 0.0)\n",
        "phase = converter.calc_phase_iq_asin2(i_signal, q_signal, isnorm=True)\n",
        "\n",
        "print(f\"\\nPhase calculation result:\")\n",
        "print(f\"  Phase: {phase}\")\n",
        "print(f\"  Contains NaN: {np.any(np.isnan(phase))}\")\n",
        "print(f\"  Length: {len(phase)}\")\n",
        "\n",
        "if not np.any(np.isnan(phase)):\n",
        "    print(\"  ✓ Phase does not contain NaN after processing\")\n",
        "else:\n",
        "    print(f\"  ✗ Phase still contains NaN: {np.sum(np.isnan(phase))} values\")\n",
        "\n",
        "print(\"\\n✓ Phase calculation with NaN test complete\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 3.4: STFT with NaN Data\n",
        "print(\"=\" * 80)\n",
        "print(\"Test 3.4: STFT Analysis with NaN Data\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "from ifi.analysis.spectrum import SpectrumAnalysis\n",
        "\n",
        "# Refine data to remove NaN\n",
        "refined_df = processing.refine_data(sample_dataframe_with_nan)\n",
        "\n",
        "print(f\"\\nData after refinement:\")\n",
        "print(f\"  Shape: {refined_df.shape}\")\n",
        "print(f\"  NaN count: {refined_df.isna().sum().sum()}\")\n",
        "\n",
        "# Calculate STFT\n",
        "analyzer = SpectrumAnalysis()\n",
        "fs_calc = 1 / refined_df[\"TIME\"].diff().mean()\n",
        "signal = refined_df[\"CH0\"].to_numpy()\n",
        "\n",
        "print(f\"\\nSTFT parameters:\")\n",
        "print(f\"  Signal length: {len(signal)}\")\n",
        "print(f\"  Sampling frequency: {fs_calc / 1e6:.2f} MHz\")\n",
        "\n",
        "# STFT should work without NaN\n",
        "freq_stft, time_stft, stft_matrix = analyzer.compute_stft(signal, fs_calc)\n",
        "\n",
        "print(f\"\\nSTFT results:\")\n",
        "print(f\"  Frequency bins: {len(freq_stft)}\")\n",
        "print(f\"  Time frames: {len(time_stft)}\")\n",
        "print(f\"  STFT matrix shape: {stft_matrix.shape}\")\n",
        "print(f\"  STFT matrix contains NaN: {np.any(np.isnan(stft_matrix))}\")\n",
        "print(f\"  Frequencies contain NaN: {np.any(np.isnan(freq_stft))}\")\n",
        "print(f\"  Times contain NaN: {np.any(np.isnan(time_stft))}\")\n",
        "\n",
        "if not np.any(np.isnan(stft_matrix)) and not np.any(np.isnan(freq_stft)) and not np.any(np.isnan(time_stft)):\n",
        "    print(\"  ✓ STFT computed successfully without NaN\")\n",
        "else:\n",
        "    print(\"  ✗ STFT contains NaN values\")\n",
        "\n",
        "print(\"\\n✓ STFT with NaN data test complete\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Enhanced Plotting Features\n",
        "\n",
        "Test enhanced plotting features with various scaling options.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 4.1: Enhanced Plotting Features\n",
        "print(\"=\" * 80)\n",
        "print(\"Test 4.1: Enhanced Plotting Features\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Test with various scaling options\n",
        "fig, axes = plotter.plot_waveforms(\n",
        "    sample_dataframe,\n",
        "    fs=50e6,\n",
        "    title=\"Enhanced Plot Test\",\n",
        "    show_plot=False,\n",
        "    time_scale=\"ms\",\n",
        "    signal_scale=\"mV\",\n",
        "    trigger_time=0.290,\n",
        "    downsample=10,\n",
        ")\n",
        "\n",
        "print(f\"\\nEnhanced plot created:\")\n",
        "print(f\"  Figure: {fig is not None}\")\n",
        "print(f\"  Number of axes: {len(axes)}\")\n",
        "\n",
        "if len(axes) > 0:\n",
        "    print(f\"\\n  Plot configuration:\")\n",
        "    for i, ax in enumerate(axes):\n",
        "        xlabel = ax.get_xlabel()\n",
        "        ylabel = ax.get_ylabel()\n",
        "        print(f\"    Axes {i}:\")\n",
        "        print(f\"      X-axis label: '{xlabel}'\")\n",
        "        print(f\"      Y-axis label: '{ylabel}'\")\n",
        "        \n",
        "        # Check if scaling is applied\n",
        "        if \"ms\" in xlabel.lower():\n",
        "            print(f\"      ✓ Time scale (ms) applied\")\n",
        "        if \"mv\" in ylabel.lower():\n",
        "            print(f\"      ✓ Signal scale (mV) applied\")\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n✓ Enhanced plotting features test complete\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cleanup\n",
        "print(\"Cleaning up temporary files...\")\n",
        "shutil.rmtree(tmp_dir, ignore_errors=True)\n",
        "print(f\"✓ Temporary directory removed: {tmp_dir}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"All integration tests completed!\")\n",
        "print(\"=\" * 80)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Database Controller Basic Tests\n",
        "\n",
        "This notebook tests the basic functionality of NAS_DB and VEST_DB controllers.\n",
        "\n",
        "## Test Objectives:\n",
        "1. **NAS_DB Tests**:\n",
        "   - Initialization and connection\n",
        "   - First call: Data loading from source (with cache creation)\n",
        "   - Second call: Data loading from cache\n",
        "   - Force remote fetch (bypass cache)\n",
        "   - File header retrieval\n",
        "\n",
        "2. **VEST_DB Tests**:\n",
        "   - Initialization and connection\n",
        "   - Shot existence check\n",
        "   - Data loading\n",
        "   - Next shot code retrieval\n",
        "\n",
        "## Configuration:\n",
        "- Shot numbers and field IDs are configurable in the cells below\n",
        "- Requires `ifi/config.ini` with proper database configuration\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cache directory: C:\\Users\\dhkdw\\Documents\\mygit\\ifi\\cache\\numba_cache\n",
            "Warning: User requested 'tbb' but it's not available\n",
            "  Using fallback threading layer: omp\n",
            "Numba threading layer: omp\n",
            "Project cache configured successfully.\n",
            "Cache configured: C:\\Users\\dhkdw\\Documents\\mygit\\ifi\\cache\\numba_cache\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO     | \n",
            "[LOGS -START] Logging started\n",
            "\n",
            "INFO     | \n",
            "[LOGS -START] All logs for this execution will be saved to: logs\\251106_144909_interactive.log\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Numba threading layer: tbb\n",
            "✓ All imports successful\n"
          ]
        }
      ],
      "source": [
        "# Setup and imports\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "# Configure Numba threading layer for parallel execution\n",
        "os.environ['NUMBA_THREADING_LAYER'] = 'tbb'\n",
        "\n",
        "# Add project root to path\n",
        "current_dir = Path.cwd()\n",
        "ifi_root = current_dir.parent if current_dir.name == \"db_controller\" else current_dir\n",
        "sys.path.insert(0, str(ifi_root))\n",
        "\n",
        "from ifi.utils.cache_setup import setup_project_cache\n",
        "cache_config = setup_project_cache()\n",
        "print(f\"Cache configured: {cache_config['cache_dir']}\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Import Numba config after setting environment variable\n",
        "try:\n",
        "    import numba\n",
        "    try:\n",
        "        numba.config.THREADING_LAYER = 'tbb'\n",
        "        print(f\"Numba threading layer: {numba.config.THREADING_LAYER}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not set Numba threading layer: {e}\")\n",
        "        print(\"Falling back to default threading layer\")\n",
        "except ImportError:\n",
        "    print(\"Warning: Numba not available\")\n",
        "\n",
        "# Import IFI modules\n",
        "from ifi.utils.common import LogManager\n",
        "from ifi.db_controller.nas_db import NAS_DB\n",
        "from ifi.db_controller.vest_db import VEST_DB\n",
        "\n",
        "# Initialize logging\n",
        "LogManager(level=\"INFO\")\n",
        "logger = LogManager().get_logger(__name__)\n",
        "\n",
        "print(\"✓ All imports successful\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Test Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Configuration:\n",
            "  - Config path: ifi/config.ini\n",
            "  - Cache folder: cache\n",
            "  - NAS shot number: 45821\n",
            "  - VEST shot number: 40656\n",
            "  - VEST field number: 109\n"
          ]
        }
      ],
      "source": [
        "# Test Configuration\n",
        "# PLEASE ADJUST THESE VALUES BASED ON YOUR 'ifi/config.ini' AND AVAILABLE DATA\n",
        "SHOT_TO_TEST = 45821  # Test for CSV files, 'MDO3000orig', 'MDO3000fetch', 'MSO58'\n",
        "# SHOT_TO_TEST = 41715 # Test for CSV files, 'MDO3000orig', 'MDO3000fetch'\n",
        "# SHOT_TO_TEST = 36853 # Test for CSV files, 'MSO58'\n",
        "# SHOT_TO_TEST = 38396 # Test for 'MDO3000pc'\n",
        "# SHOT_TO_TEST = 'AGC w attn I' # Test for 'ETC'\n",
        "\n",
        "CONFIG_PATH = 'ifi/config.ini'\n",
        "CACHE_FOLDER = Path('./cache')  # Should match [LOCAL_CACHE] dumping_folder in config\n",
        "\n",
        "VEST_SHOT_TO_TEST = 40656\n",
        "VEST_FIELD_TO_TEST = 109\n",
        "\n",
        "print(f\"Test Configuration:\")\n",
        "print(f\"  - Config path: {CONFIG_PATH}\")\n",
        "print(f\"  - Cache folder: {CACHE_FOLDER}\")\n",
        "print(f\"  - NAS shot number: {SHOT_TO_TEST}\")\n",
        "print(f\"  - VEST shot number: {VEST_SHOT_TO_TEST}\")\n",
        "print(f\"  - VEST field number: {VEST_FIELD_TO_TEST}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. NAS_DB Tests\n",
        "\n",
        "### Test 1: First Call - Fetch and Cache\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[3], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m NAS_DB(config_path\u001b[38;5;241m=\u001b[39mCONFIG_PATH) \u001b[38;5;28;01mas\u001b[39;00m nas:\n\u001b[1;32m---> 15\u001b[0m         data_dict \u001b[38;5;241m=\u001b[39m \u001b[43mnas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_shot_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSHOT_TO_TEST\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_dict, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data_dict) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     18\u001b[0m             logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   -> Data loaded successfully on first call.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\dhkdw\\Documents\\mygit\\ifi\\ifi\\db_controller\\nas_db.py:643\u001b[0m, in \u001b[0;36mNAS_DB.get_shot_data\u001b[1;34m(self, query, data_folders, add_path, force_remote, **kwargs)\u001b[0m\n\u001b[0;32m    639\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlog_tag(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNASDB\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCACHE\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m -> Key FOUND in cache. Loading from HDF5.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    640\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[0;32m    641\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlog_tag(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNASDB\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCACHE\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Found \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbasename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m in cache: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcache_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    642\u001b[0m )\n\u001b[1;32m--> 643\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_hdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcache_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    645\u001b[0m \u001b[38;5;66;03m# Restore metadata if available\u001b[39;00m\n\u001b[0;32m    646\u001b[0m metadata_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\dhkdw\\Documents\\mygit\\ifi\\.venv\\lib\\site-packages\\pandas\\io\\pytables.py:465\u001b[0m, in \u001b[0;36mread_hdf\u001b[1;34m(path_or_buf, key, mode, errors, where, start, stop, columns, iterator, chunksize, **kwargs)\u001b[0m\n\u001b[0;32m    460\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    461\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkey must be provided when HDF5 \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    462\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile contains multiple datasets.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    463\u001b[0m                 )\n\u001b[0;32m    464\u001b[0m         key \u001b[38;5;241m=\u001b[39m candidate_only_group\u001b[38;5;241m.\u001b[39m_v_pathname\n\u001b[1;32m--> 465\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    466\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    468\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    469\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    470\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    471\u001b[0m \u001b[43m        \u001b[49m\u001b[43miterator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    472\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    473\u001b[0m \u001b[43m        \u001b[49m\u001b[43mauto_close\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauto_close\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mLookupError\u001b[39;00m):\n\u001b[0;32m    476\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, HDFStore):\n\u001b[0;32m    477\u001b[0m         \u001b[38;5;66;03m# if there is an error, close the store if we opened it.\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\dhkdw\\Documents\\mygit\\ifi\\.venv\\lib\\site-packages\\pandas\\io\\pytables.py:919\u001b[0m, in \u001b[0;36mHDFStore.select\u001b[1;34m(self, key, where, start, stop, columns, iterator, chunksize, auto_close)\u001b[0m\n\u001b[0;32m    905\u001b[0m \u001b[38;5;66;03m# create the iterator\u001b[39;00m\n\u001b[0;32m    906\u001b[0m it \u001b[38;5;241m=\u001b[39m TableIterator(\n\u001b[0;32m    907\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    908\u001b[0m     s,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    916\u001b[0m     auto_close\u001b[38;5;241m=\u001b[39mauto_close,\n\u001b[0;32m    917\u001b[0m )\n\u001b[1;32m--> 919\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\dhkdw\\Documents\\mygit\\ifi\\.venv\\lib\\site-packages\\pandas\\io\\pytables.py:2042\u001b[0m, in \u001b[0;36mTableIterator.get_result\u001b[1;34m(self, coordinates)\u001b[0m\n\u001b[0;32m   2039\u001b[0m     where \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwhere\n\u001b[0;32m   2041\u001b[0m \u001b[38;5;66;03m# directly return the result\u001b[39;00m\n\u001b[1;32m-> 2042\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2043\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m   2044\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
            "File \u001b[1;32mc:\\Users\\dhkdw\\Documents\\mygit\\ifi\\.venv\\lib\\site-packages\\pandas\\io\\pytables.py:903\u001b[0m, in \u001b[0;36mHDFStore.select.<locals>.func\u001b[1;34m(_start, _stop, _where)\u001b[0m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfunc\u001b[39m(_start, _stop, _where):\n\u001b[1;32m--> 903\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_stop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_where\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\dhkdw\\Documents\\mygit\\ifi\\.venv\\lib\\site-packages\\pandas\\io\\pytables.py:4712\u001b[0m, in \u001b[0;36mAppendableFrameTable.read\u001b[1;34m(self, where, columns, start, stop)\u001b[0m\n\u001b[0;32m   4709\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer_axes():\n\u001b[0;32m   4710\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 4712\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_axes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4714\u001b[0m info \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   4715\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnon_index_axes[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m], {})\n\u001b[0;32m   4716\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnon_index_axes)\n\u001b[0;32m   4717\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[0;32m   4718\u001b[0m )\n\u001b[0;32m   4720\u001b[0m inds \u001b[38;5;241m=\u001b[39m [i \u001b[38;5;28;01mfor\u001b[39;00m i, ax \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes) \u001b[38;5;28;01mif\u001b[39;00m ax \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex_axes[\u001b[38;5;241m0\u001b[39m]]\n",
            "File \u001b[1;32mc:\\Users\\dhkdw\\Documents\\mygit\\ifi\\.venv\\lib\\site-packages\\pandas\\io\\pytables.py:3895\u001b[0m, in \u001b[0;36mTable._read_axes\u001b[1;34m(self, where, start, stop)\u001b[0m\n\u001b[0;32m   3893\u001b[0m \u001b[38;5;66;03m# create the selection\u001b[39;00m\n\u001b[0;32m   3894\u001b[0m selection \u001b[38;5;241m=\u001b[39m Selection(\u001b[38;5;28mself\u001b[39m, where\u001b[38;5;241m=\u001b[39mwhere, start\u001b[38;5;241m=\u001b[39mstart, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[1;32m-> 3895\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[43mselection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3897\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   3898\u001b[0m \u001b[38;5;66;03m# convert the data\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\dhkdw\\Documents\\mygit\\ifi\\.venv\\lib\\site-packages\\pandas\\io\\pytables.py:5508\u001b[0m, in \u001b[0;36mSelection.select\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   5506\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoordinates \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5507\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtable\u001b[38;5;241m.\u001b[39mtable\u001b[38;5;241m.\u001b[39mread_coordinates(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoordinates)\n\u001b[1;32m-> 5508\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\dhkdw\\Documents\\mygit\\ifi\\.venv\\lib\\site-packages\\tables\\table.py:1984\u001b[0m, in \u001b[0;36mTable.read\u001b[1;34m(self, start, stop, step, field, out)\u001b[0m\n\u001b[0;32m   1979\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m   1981\u001b[0m start, stop, step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_range(start, stop, step,\n\u001b[0;32m   1982\u001b[0m                                         warn_negstep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m-> 1984\u001b[0m arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfield\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1985\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m internal_to_flavor(arr, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflavor)\n",
            "File \u001b[1;32mc:\\Users\\dhkdw\\Documents\\mygit\\ifi\\.venv\\lib\\site-packages\\tables\\table.py:1893\u001b[0m, in \u001b[0;36mTable._read\u001b[1;34m(self, start, stop, step, field, out)\u001b[0m\n\u001b[0;32m   1889\u001b[0m \u001b[38;5;66;03m# Call the routine to fill-up the resulting array\u001b[39;00m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m field:\n\u001b[0;32m   1891\u001b[0m     \u001b[38;5;66;03m# This optimization works three times faster than\u001b[39;00m\n\u001b[0;32m   1892\u001b[0m     \u001b[38;5;66;03m# the row._fill_col method (up to 170 MB/s on a pentium IV @ 2GHz)\u001b[39;00m\n\u001b[1;32m-> 1893\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_records\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1894\u001b[0m \u001b[38;5;66;03m# Warning!: _read_field_name should not be used until\u001b[39;00m\n\u001b[0;32m   1895\u001b[0m \u001b[38;5;66;03m# H5TBread_fields_name in tableextension will be finished\u001b[39;00m\n\u001b[0;32m   1896\u001b[0m \u001b[38;5;66;03m# F. Alted 2005/05/26\u001b[39;00m\n\u001b[0;32m   1897\u001b[0m \u001b[38;5;66;03m# XYX Ho implementem per a PyTables 2.0??\u001b[39;00m\n\u001b[0;32m   1898\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m field \u001b[38;5;129;01mand\u001b[39;00m step \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m15\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1899\u001b[0m     \u001b[38;5;66;03m# For step>15, this seems to work always faster than row._fill_col.\u001b[39;00m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Test 1: First call - Data is not cached. It will be fetched from the remote source\n",
        "# and then saved to the local cache (e.g., './cache/45821.h5').\n",
        "logger.info(\"=\" * 60)\n",
        "logger.info(\"NAS_DB Basic Functionality Test\")\n",
        "logger.info(\"=\" * 60)\n",
        "\n",
        "if not Path(CONFIG_PATH).exists():\n",
        "    logger.error(f\"Configuration file not found at '{CONFIG_PATH}'\")\n",
        "    logger.error(\"Please create it from the template\")\n",
        "else:\n",
        "    logger.info(\"\\n--- Test 1: First call to get_shot_data (should fetch and cache) ---\")\n",
        "    \n",
        "    try:\n",
        "        with NAS_DB(config_path=CONFIG_PATH) as nas:\n",
        "            data_dict = nas.get_shot_data(SHOT_TO_TEST)\n",
        "            \n",
        "            if data_dict and isinstance(data_dict, dict) and len(data_dict) > 0:\n",
        "                logger.info(\"   -> Data loaded successfully on first call.\")\n",
        "                logger.info(f\"   -> Number of dataframes: {len(data_dict)}\")\n",
        "                for key, df in data_dict.items():\n",
        "                    logger.info(f\"   -> DataFrame '{key}' shape: {df.shape}\")\n",
        "                    logger.info(f\"      - Columns: {df.columns.tolist()}\")\n",
        "                \n",
        "                # Display first file head\n",
        "                first_df = list(data_dict.values())[0]\n",
        "                print(\"\\n--- Data Head (first file) ---\")\n",
        "                print(first_df.head())\n",
        "                print(\"------------------------------\")\n",
        "            else:\n",
        "                logger.error(\"   -> FAILED: Could not retrieve any data on the first call.\")\n",
        "                logger.warning(\"   This may be expected if shot number is not available or NAS is not accessible\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to connect to NAS or load data: {e}\", exc_info=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test 2: Second Call - Load from Cache\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 2: Second call - The local cache file now exists. This call should be much faster\n",
        "# as it reads directly from the local HDF5 file.\n",
        "logger.info(\"\\n--- Test 2: Second call to get_shot_data (should load from cache) ---\")\n",
        "\n",
        "try:\n",
        "    with NAS_DB(config_path=CONFIG_PATH) as nas:\n",
        "        cached_data_dict = nas.get_shot_data(SHOT_TO_TEST)\n",
        "        \n",
        "        if cached_data_dict and isinstance(cached_data_dict, dict):\n",
        "            logger.info(\"   -> Data loaded successfully from cache.\")\n",
        "            logger.info(f\"   -> Number of dataframes: {len(cached_data_dict)}\")\n",
        "            for key, df in cached_data_dict.items():\n",
        "                logger.info(f\"   -> DataFrame '{key}' shape: {df.shape}\")\n",
        "            \n",
        "            if len(cached_data_dict) == len(data_dict):\n",
        "                logger.info(\"   -> SUCCESS: Number of cached files matches original files.\")\n",
        "            else:\n",
        "                logger.error(f\"   -> FAILED: Number mismatch: original={len(data_dict)}, cached={len(cached_data_dict)}\")\n",
        "        else:\n",
        "            logger.error(\"   -> FAILED: Could not retrieve data from cache or result is not a dictionary.\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Failed to load from cache: {e}\", exc_info=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test 3: Verify Cache File Creation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 3: Verify cache file creation\n",
        "cache_dir = CACHE_FOLDER / str(SHOT_TO_TEST)\n",
        "expected_cache_file = cache_dir / f\"{SHOT_TO_TEST}.h5\"\n",
        "\n",
        "logger.info(\"\\n--- Test 3: Verifying cache file creation ---\")\n",
        "if expected_cache_file.exists():\n",
        "    logger.info(f\"   -> SUCCESS: Cache file created at '{expected_cache_file}'\")\n",
        "    cache_size = expected_cache_file.stat().st_size\n",
        "    logger.info(f\"      - Cache file size: {cache_size:,} bytes\")\n",
        "else:\n",
        "    logger.warning(f\"   -> WARNING: Cache file was not created at '{expected_cache_file}'.\")\n",
        "    logger.warning(\"   This may be expected if caching is disabled or connection failed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test 4: Force Remote Fetch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 4: Force remote fetch - Use 'force_remote=True' to bypass the local cache and\n",
        "# re-download the data from the source. This is useful if the data has changed.\n",
        "logger.info(\"\\n--- Test 4: Third call with force_remote=True (should bypass cache) ---\")\n",
        "\n",
        "try:\n",
        "    with NAS_DB(config_path=CONFIG_PATH) as nas:\n",
        "        forced_data_dict = nas.get_shot_data(SHOT_TO_TEST, force_remote=True)\n",
        "        \n",
        "        if forced_data_dict and isinstance(forced_data_dict, dict):\n",
        "            logger.info(\"   -> Data loaded successfully by forcing remote fetch.\")\n",
        "            logger.info(f\"   -> Number of dataframes: {len(forced_data_dict)}\")\n",
        "            for key, df in forced_data_dict.items():\n",
        "                logger.info(f\"   -> DataFrame '{key}' shape: {df.shape}\")\n",
        "        else:\n",
        "            logger.warning(\"   -> WARNING: Could not load data with force_remote=True.\")\n",
        "            logger.warning(\"   This may be expected if remote connection is not available\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Failed to force remote fetch: {e}\", exc_info=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test 5: File Header Retrieval\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- File Head ---\n",
            "Model,MDO3014\n",
            "Firmware Version,empty\n",
            ",\n",
            "Waveform Type,empty\n",
            "Point Format,Y\n",
            "Horizontal Units,s\n",
            "Horizontal Scale,empty\n",
            "Horizontal Delay,empty\n",
            "Sample Interval,4e-09\n",
            "Record Length,10000000\n",
            "Gating,empty\n",
            "Probe Attenuation,empty,empty,empty\n",
            "Vertical Units,V,V,V\n",
            "Vertical Offset,-2,0.0026,-0.0124\n",
            "Vertical Scale,empty,empty,empty\n",
            "Vertical Position,empty,empty,empty\n",
            ",,,\n",
            ",,,\n",
            ",,,\n",
            "Label,,,\n",
            "TIME,CH1,CH2,CH3\n",
            "-0.01,-0.48,0.0186,0.014\n",
            "-0.009999996,-0.72,0.0206,0.0156\n",
            "-0.009999992,-0.72,0.021,0.0184\n",
            "-0.009999988,-0.56,0.0206,0.0196\n",
            "-0.009999984,-0.16,0.019,0.0208\n",
            "-0.00999998,0.16,0.0174,0.02\n",
            "-0.009999976,0.4,0.0158,0.0196\n",
            "-0.009999972,0.72,0.0142,0.0172\n",
            "-0.009999968,0.72,0.0126,0.0164\n",
            "-0.009999964,0.56,0.0134,0.0144\n",
            "-0.00999996,0.4,0.015,0.0132\n",
            "-0.009999956,0,0.0158,0.0132\n",
            "-0.009999952,-0.32,0.0174,0.0132\n",
            "-0.009999948,-0.64,0.0198,0.0152\n",
            "-0.009999944,-0.72,0.0198,0.0172\n",
            "-0.00999994,-0.72,0.021,0.0192\n",
            "-0.009999936,-0.4,0.0198,0.0204\n",
            "-0.009999932,0,0.0186,0.0204\n",
            "-0.009999928,0.4,0.017,0.0196\n",
            "-0.009999924,0.72,0.015,0.0188\n",
            "-0.00999992,0.72,0.013,0.0168\n",
            "-0.009999916,0.72,0.0134,0.0152\n",
            "-0.009999912,0.56,0.0138,0.0132\n",
            "-0.009999908,0.16,0.0154,0.0124\n",
            "-0.009999904,-0.24,0.0166,0.0132\n",
            "-0.0099999,-0.56,0.019,0.014\n",
            "-0.009999896,-0.64,0.0202,0.0164\n",
            "-0.009999892,-0.8,0.0206,0.0184\n",
            "-0.009999888,-0.56,0.0206,0.0196\n",
            "\n",
            "-----------------\n"
          ]
        }
      ],
      "source": [
        "# Test 5: File header retrieval\n",
        "logger.info(f\"\\n--- Test 5: Fetching top lines for shot #{SHOT_TO_TEST}... ---\")\n",
        "\n",
        "try:\n",
        "    with NAS_DB(config_path=CONFIG_PATH) as nas:\n",
        "        file_head = nas.get_data_top(SHOT_TO_TEST, lines=50)\n",
        "        \n",
        "        if file_head:\n",
        "            logger.info(\"   -> SUCCESS: Retrieved file head.\")\n",
        "            print(\"\\n--- File Head ---\")\n",
        "            print(file_head)\n",
        "            print(\"-----------------\")\n",
        "        else:\n",
        "            logger.warning(\"   -> WARNING: Could not retrieve file head.\")\n",
        "            logger.warning(\"   This may be expected if method is not available or file not found\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Failed to retrieve file head: {e}\", exc_info=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. VEST_DB Tests\n",
        "\n",
        "### Test 1: Check Shot Existence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 1: Check if shot exists\n",
        "logger.info(\"=\" * 60)\n",
        "logger.info(\"VEST_DB Basic Functionality Test\")\n",
        "logger.info(\"=\" * 60)\n",
        "\n",
        "if not Path(CONFIG_PATH).exists():\n",
        "    logger.error(f\"Configuration file not found at '{CONFIG_PATH}'\")\n",
        "    logger.error(\"Please create it from the template\")\n",
        "else:\n",
        "    logger.info(f\"\\n--- Test 1: Checking for existence of VEST shot #{VEST_SHOT_TO_TEST}, Field #{VEST_FIELD_TO_TEST}... ---\")\n",
        "    \n",
        "    try:\n",
        "        with VEST_DB(config_path=CONFIG_PATH) as db:\n",
        "            existence = db.exist_shot(VEST_SHOT_TO_TEST, VEST_FIELD_TO_TEST)\n",
        "            \n",
        "            if existence > 0:\n",
        "                logger.info(f\"   -> SUCCESS: Shot found in table (shotDataWaveform_{existence}).\")\n",
        "            else:\n",
        "                logger.warning(f\"   -> WARNING: Shot #{VEST_SHOT_TO_TEST} not found in VEST DB.\")\n",
        "                logger.warning(\"   This may be expected if test shot number doesn't exist\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to connect to VEST database: {e}\", exc_info=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test 2: Load Shot Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Head of DataFrame (25k) ---\n",
            "         Ip_raw ([V])\n",
            "0.00000      0.015717\n",
            "0.00004      0.018311\n",
            "0.00008      0.020142\n",
            "0.00012      0.018921\n",
            "0.00016      0.019836\n",
            "-------------------------\n"
          ]
        }
      ],
      "source": [
        "# Test 2: Load shot data\n",
        "logger.info(f\"\\n--- Test 2: Attempting to load data for shot #{VEST_SHOT_TO_TEST}, Field #{VEST_FIELD_TO_TEST}... ---\")\n",
        "\n",
        "try:\n",
        "    with VEST_DB(config_path=CONFIG_PATH) as db:\n",
        "        result_dict = db.load_shot(VEST_SHOT_TO_TEST, [VEST_FIELD_TO_TEST])\n",
        "        \n",
        "        if result_dict and isinstance(result_dict, dict) and len(result_dict) > 0:\n",
        "            logger.info(\"   -> SUCCESS: VEST data loaded successfully.\")\n",
        "            for rate_key, df in result_dict.items():\n",
        "                logger.info(f\"   -> Rate group '{rate_key}':\")\n",
        "                logger.info(f\"      - DataFrame shape: {df.shape}\")\n",
        "                logger.info(f\"      - Columns: {df.columns.tolist()}\")\n",
        "                \n",
        "                print(f\"\\n--- Head of DataFrame ({rate_key}) ---\")\n",
        "                print(df.head())\n",
        "                print(\"-------------------------\")\n",
        "        else:\n",
        "            logger.error(\"   -> FAILED: Could not load VEST data or result is empty.\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Failed to load VEST data: {e}\", exc_info=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test 3: Get Next Shot Code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 3: Get next shot code\n",
        "logger.info(\"\\n--- Test 3: Getting next shot code... ---\")\n",
        "\n",
        "try:\n",
        "    with VEST_DB(config_path=CONFIG_PATH) as db:\n",
        "        next_shot = db.get_next_shot_code()\n",
        "        if next_shot:\n",
        "            logger.info(f\"   -> SUCCESS: Next shot code: {next_shot}\")\n",
        "        else:\n",
        "            logger.warning(\"   -> WARNING: Could not get next shot code.\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Failed to get next shot code: {e}\", exc_info=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Test Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test Summary\n",
        "logger.info(\"\\n\" + \"=\" * 60)\n",
        "logger.info(\"Test Summary\")\n",
        "logger.info(\"=\" * 60)\n",
        "logger.info(\"\\nAll tests completed. Check the logs above for detailed results.\")\n",
        "logger.info(\"\\nNote: Some tests may fail if database connections are not available.\")\n",
        "logger.info(\"This is expected in test environments.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Database Controller Basic Tests\n",
        "\n",
        "This notebook tests the basic functionality of NAS_DB and VEST_DB controllers.\n",
        "\n",
        "## Test Objectives:\n",
        "1. **NAS_DB Tests**:\n",
        "   - Initialization and connection\n",
        "   - First call: Data loading from source (with cache creation)\n",
        "   - Second call: Data loading from cache\n",
        "   - Force remote fetch (bypass cache)\n",
        "   - File header retrieval\n",
        "\n",
        "2. **VEST_DB Tests**:\n",
        "   - Initialization and connection\n",
        "   - Shot existence check\n",
        "   - Data loading\n",
        "   - Next shot code retrieval\n",
        "\n",
        "## Configuration:\n",
        "- Shot numbers and field IDs are configurable in the cells below\n",
        "- Requires `ifi/config.ini` with proper database configuration\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup and imports\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "# Configure Numba threading layer for parallel execution\n",
        "os.environ['NUMBA_THREADING_LAYER'] = 'tbb'\n",
        "\n",
        "# Add project root to path\n",
        "current_dir = Path.cwd()\n",
        "ifi_root = current_dir.parent if current_dir.name == \"db_controller\" else current_dir\n",
        "sys.path.insert(0, str(ifi_root))\n",
        "\n",
        "from ifi.utils.cache_setup import setup_project_cache\n",
        "cache_config = setup_project_cache()\n",
        "print(f\"Cache configured: {cache_config['cache_dir']}\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Import Numba config after setting environment variable\n",
        "try:\n",
        "    import numba\n",
        "    try:\n",
        "        numba.config.THREADING_LAYER = 'tbb'\n",
        "        print(f\"Numba threading layer: {numba.config.THREADING_LAYER}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not set Numba threading layer: {e}\")\n",
        "        print(\"Falling back to default threading layer\")\n",
        "except ImportError:\n",
        "    print(\"Warning: Numba not available\")\n",
        "\n",
        "# Import IFI modules\n",
        "from ifi.utils.common import LogManager\n",
        "from ifi.db_controller.nas_db import NAS_DB\n",
        "from ifi.db_controller.vest_db import VEST_DB\n",
        "\n",
        "# Initialize logging\n",
        "LogManager(level=\"INFO\")\n",
        "logger = LogManager().get_logger(__name__)\n",
        "\n",
        "print(\"âœ“ All imports successful\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Test Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test Configuration\n",
        "# PLEASE ADJUST THESE VALUES BASED ON YOUR 'ifi/config.ini' AND AVAILABLE DATA\n",
        "SHOT_TO_TEST = 45821  # Test for CSV files, 'MDO3000orig', 'MDO3000fetch', 'MSO58'\n",
        "# SHOT_TO_TEST = 41715 # Test for CSV files, 'MDO3000orig', 'MDO3000fetch'\n",
        "# SHOT_TO_TEST = 36853 # Test for CSV files, 'MSO58'\n",
        "# SHOT_TO_TEST = 38396 # Test for 'MDO3000pc'\n",
        "# SHOT_TO_TEST = 'AGC w attn I' # Test for 'ETC'\n",
        "\n",
        "CONFIG_PATH = 'ifi/config.ini'\n",
        "CACHE_FOLDER = Path('./cache')  # Should match [LOCAL_CACHE] dumping_folder in config\n",
        "\n",
        "VEST_SHOT_TO_TEST = 40656\n",
        "VEST_FIELD_TO_TEST = 109\n",
        "\n",
        "print(f\"Test Configuration:\")\n",
        "print(f\"  - Config path: {CONFIG_PATH}\")\n",
        "print(f\"  - Cache folder: {CACHE_FOLDER}\")\n",
        "print(f\"  - NAS shot number: {SHOT_TO_TEST}\")\n",
        "print(f\"  - VEST shot number: {VEST_SHOT_TO_TEST}\")\n",
        "print(f\"  - VEST field number: {VEST_FIELD_TO_TEST}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. NAS_DB Tests\n",
        "\n",
        "### Test 1: First Call - Fetch and Cache\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 1: First call - Data is not cached. It will be fetched from the remote source\n",
        "# and then saved to the local cache (e.g., './cache/45821.h5').\n",
        "logger.info(\"=\" * 60)\n",
        "logger.info(\"NAS_DB Basic Functionality Test\")\n",
        "logger.info(\"=\" * 60)\n",
        "\n",
        "if not Path(CONFIG_PATH).exists():\n",
        "    logger.error(f\"Configuration file not found at '{CONFIG_PATH}'\")\n",
        "    logger.error(\"Please create it from the template\")\n",
        "else:\n",
        "    logger.info(\"\\n--- Test 1: First call to get_shot_data (should fetch and cache) ---\")\n",
        "    \n",
        "    try:\n",
        "        with NAS_DB(config_path=CONFIG_PATH) as nas:\n",
        "            data_dict = nas.get_shot_data(SHOT_TO_TEST)\n",
        "            \n",
        "            if data_dict and isinstance(data_dict, dict) and len(data_dict) > 0:\n",
        "                logger.info(\"   -> Data loaded successfully on first call.\")\n",
        "                logger.info(f\"   -> Number of dataframes: {len(data_dict)}\")\n",
        "                for key, df in data_dict.items():\n",
        "                    logger.info(f\"   -> DataFrame '{key}' shape: {df.shape}\")\n",
        "                    logger.info(f\"      - Columns: {df.columns.tolist()}\")\n",
        "                \n",
        "                # Display first file head\n",
        "                first_df = list(data_dict.values())[0]\n",
        "                print(\"\\n--- Data Head (first file) ---\")\n",
        "                print(first_df.head())\n",
        "                print(\"------------------------------\")\n",
        "            else:\n",
        "                logger.error(\"   -> FAILED: Could not retrieve any data on the first call.\")\n",
        "                logger.warning(\"   This may be expected if shot number is not available or NAS is not accessible\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to connect to NAS or load data: {e}\", exc_info=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test 2: Second Call - Load from Cache\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 2: Second call - The local cache file now exists. This call should be much faster\n",
        "# as it reads directly from the local HDF5 file.\n",
        "logger.info(\"\\n--- Test 2: Second call to get_shot_data (should load from cache) ---\")\n",
        "\n",
        "try:\n",
        "    with NAS_DB(config_path=CONFIG_PATH) as nas:\n",
        "        cached_data_dict = nas.get_shot_data(SHOT_TO_TEST)\n",
        "        \n",
        "        if cached_data_dict and isinstance(cached_data_dict, dict):\n",
        "            logger.info(\"   -> Data loaded successfully from cache.\")\n",
        "            logger.info(f\"   -> Number of dataframes: {len(cached_data_dict)}\")\n",
        "            for key, df in cached_data_dict.items():\n",
        "                logger.info(f\"   -> DataFrame '{key}' shape: {df.shape}\")\n",
        "            \n",
        "            if len(cached_data_dict) == len(data_dict):\n",
        "                logger.info(\"   -> SUCCESS: Number of cached files matches original files.\")\n",
        "            else:\n",
        "                logger.error(f\"   -> FAILED: Number mismatch: original={len(data_dict)}, cached={len(cached_data_dict)}\")\n",
        "        else:\n",
        "            logger.error(\"   -> FAILED: Could not retrieve data from cache or result is not a dictionary.\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Failed to load from cache: {e}\", exc_info=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test 3: Verify Cache File Creation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 3: Verify cache file creation\n",
        "cache_dir = CACHE_FOLDER / str(SHOT_TO_TEST)\n",
        "expected_cache_file = cache_dir / f\"{SHOT_TO_TEST}.h5\"\n",
        "\n",
        "logger.info(\"\\n--- Test 3: Verifying cache file creation ---\")\n",
        "if expected_cache_file.exists():\n",
        "    logger.info(f\"   -> SUCCESS: Cache file created at '{expected_cache_file}'\")\n",
        "    cache_size = expected_cache_file.stat().st_size\n",
        "    logger.info(f\"      - Cache file size: {cache_size:,} bytes\")\n",
        "else:\n",
        "    logger.warning(f\"   -> WARNING: Cache file was not created at '{expected_cache_file}'.\")\n",
        "    logger.warning(\"   This may be expected if caching is disabled or connection failed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test 4: Force Remote Fetch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 4: Force remote fetch - Use 'force_remote=True' to bypass the local cache and\n",
        "# re-download the data from the source. This is useful if the data has changed.\n",
        "logger.info(\"\\n--- Test 4: Third call with force_remote=True (should bypass cache) ---\")\n",
        "\n",
        "try:\n",
        "    with NAS_DB(config_path=CONFIG_PATH) as nas:\n",
        "        forced_data_dict = nas.get_shot_data(SHOT_TO_TEST, force_remote=True)\n",
        "        \n",
        "        if forced_data_dict and isinstance(forced_data_dict, dict):\n",
        "            logger.info(\"   -> Data loaded successfully by forcing remote fetch.\")\n",
        "            logger.info(f\"   -> Number of dataframes: {len(forced_data_dict)}\")\n",
        "            for key, df in forced_data_dict.items():\n",
        "                logger.info(f\"   -> DataFrame '{key}' shape: {df.shape}\")\n",
        "        else:\n",
        "            logger.warning(\"   -> WARNING: Could not load data with force_remote=True.\")\n",
        "            logger.warning(\"   This may be expected if remote connection is not available\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Failed to force remote fetch: {e}\", exc_info=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test 5: File Header Retrieval\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 5: File header retrieval\n",
        "logger.info(f\"\\n--- Test 5: Fetching top lines for shot #{SHOT_TO_TEST}... ---\")\n",
        "\n",
        "try:\n",
        "    with NAS_DB(config_path=CONFIG_PATH) as nas:\n",
        "        file_head = nas.get_data_top(SHOT_TO_TEST, lines=50)\n",
        "        \n",
        "        if file_head:\n",
        "            logger.info(\"   -> SUCCESS: Retrieved file head.\")\n",
        "            print(\"\\n--- File Head ---\")\n",
        "            print(file_head)\n",
        "            print(\"-----------------\")\n",
        "        else:\n",
        "            logger.warning(\"   -> WARNING: Could not retrieve file head.\")\n",
        "            logger.warning(\"   This may be expected if method is not available or file not found\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Failed to retrieve file head: {e}\", exc_info=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. VEST_DB Tests\n",
        "\n",
        "### Test 1: Check Shot Existence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 1: Check if shot exists\n",
        "logger.info(\"=\" * 60)\n",
        "logger.info(\"VEST_DB Basic Functionality Test\")\n",
        "logger.info(\"=\" * 60)\n",
        "\n",
        "if not Path(CONFIG_PATH).exists():\n",
        "    logger.error(f\"Configuration file not found at '{CONFIG_PATH}'\")\n",
        "    logger.error(\"Please create it from the template\")\n",
        "else:\n",
        "    logger.info(f\"\\n--- Test 1: Checking for existence of VEST shot #{VEST_SHOT_TO_TEST}, Field #{VEST_FIELD_TO_TEST}... ---\")\n",
        "    \n",
        "    try:\n",
        "        with VEST_DB(config_path=CONFIG_PATH) as db:\n",
        "            existence = db.exist_shot(VEST_SHOT_TO_TEST, VEST_FIELD_TO_TEST)\n",
        "            \n",
        "            if existence > 0:\n",
        "                logger.info(f\"   -> SUCCESS: Shot found in table (shotDataWaveform_{existence}).\")\n",
        "            else:\n",
        "                logger.warning(f\"   -> WARNING: Shot #{VEST_SHOT_TO_TEST} not found in VEST DB.\")\n",
        "                logger.warning(\"   This may be expected if test shot number doesn't exist\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to connect to VEST database: {e}\", exc_info=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test 2: Load Shot Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 2: Load shot data\n",
        "logger.info(f\"\\n--- Test 2: Attempting to load data for shot #{VEST_SHOT_TO_TEST}, Field #{VEST_FIELD_TO_TEST}... ---\")\n",
        "\n",
        "try:\n",
        "    with VEST_DB(config_path=CONFIG_PATH) as db:\n",
        "        result_dict = db.load_shot(VEST_SHOT_TO_TEST, [VEST_FIELD_TO_TEST])\n",
        "        \n",
        "        if result_dict and isinstance(result_dict, dict) and len(result_dict) > 0:\n",
        "            logger.info(\"   -> SUCCESS: VEST data loaded successfully.\")\n",
        "            for rate_key, df in result_dict.items():\n",
        "                logger.info(f\"   -> Rate group '{rate_key}':\")\n",
        "                logger.info(f\"      - DataFrame shape: {df.shape}\")\n",
        "                logger.info(f\"      - Columns: {df.columns.tolist()}\")\n",
        "                \n",
        "                print(f\"\\n--- Head of DataFrame ({rate_key}) ---\")\n",
        "                print(df.head())\n",
        "                print(\"-------------------------\")\n",
        "        else:\n",
        "            logger.error(\"   -> FAILED: Could not load VEST data or result is empty.\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Failed to load VEST data: {e}\", exc_info=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test 3: Get Next Shot Code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 3: Get next shot code\n",
        "logger.info(\"\\n--- Test 3: Getting next shot code... ---\")\n",
        "\n",
        "try:\n",
        "    with VEST_DB(config_path=CONFIG_PATH) as db:\n",
        "        next_shot = db.get_next_shot_code()\n",
        "        if next_shot:\n",
        "            logger.info(f\"   -> SUCCESS: Next shot code: {next_shot}\")\n",
        "        else:\n",
        "            logger.warning(\"   -> WARNING: Could not get next shot code.\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Failed to get next shot code: {e}\", exc_info=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Test Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test Summary\n",
        "logger.info(\"\\n\" + \"=\" * 60)\n",
        "logger.info(\"Test Summary\")\n",
        "logger.info(\"=\" * 60)\n",
        "logger.info(\"\\nAll tests completed. Check the logs above for detailed results.\")\n",
        "logger.info(\"\\nNote: Some tests may fail if database connections are not available.\")\n",
        "logger.info(\"This is expected in test environments.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
